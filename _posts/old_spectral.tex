\chapter{Classical Scaling and Spectral Approaches}
\label{chap:spectral}

\fixme{Introduce isomap using the cities example to describe it ...}



\section{Distances along the Manifold}

Outline  


\subsection{Data}

Data

%
\begin{figure}
\subfigure[\texttt{\small 'plane'}]{

\includegraphics[width=0.32\textwidth]{../diagrams/planeData}} \subfigure[\texttt{\small 'swissroll'}]{

\includegraphics[width=0.32\textwidth]{../diagrams/swissrollData}}\subfigure[\texttt{\small 'trefoil'}]{

\includegraphics[width=0.32\textwidth]{../diagrams/trefoilData}}

\caption{Illustrative data sets for the talk. Each data set is generated by
calling \texttt{generateManifoldData(dataType)}. The \texttt{dataType}
argument is given below each plot.}

\end{figure}



\section{Isomap}


\begin{itemize}
\item \emph{\cite{Tenenbaum:isomap00}}
\item MDS finds geometric configuration preserving distances 
\item MDS applied to Manifold distance 
\item Geodesic Distance = Manifold Distance 
\item Cannot compute geodesic distance without knowing manifold 
\end{itemize}
Isomap
\begin{itemize}
\item Isomap: define neighbors and compute distances between neighbours.
\item Geodesic Distance approximated by shortest path through adjacency
matrix. 
\end{itemize}
\includegraphics[width=10cm]{../diagrams/carl/isomap_geodesic}

Isomap Examples%
\footnote{Data generation Carl Henrik Ek%
}%
\begin{figure}


\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/planeData}}\hfill{}

\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/demIsomapPlane}}

\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/swissrollData}}\hfill{}\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/demIsomapSwissroll}}

\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/trefoilData}}\hfill{}\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/demIsomapTrefoil}}\caption{Figure}

\end{figure}


\texttt{demIsomap}

\subsection{Isomap: Summary}
\begin{itemize}
\item MDS on shortest path approximation of manifold distance 
\item [+] Simple 
\item [+] Intrinsic dimension from eigen spectra 
\item [-] Solves a very large eigenvalue problem 
\item [-] Cannot handle holes or non-convex manifold 
\item [-] Sensitive to {}``short circuit'' 
\end{itemize}

\section{Maximum Variance Unfolding}
\begin{itemize}
\item Compute neighborhood, constrain local distances to be preserved.

\begin{itemize}
\item Maximise the variance in latent space.
\end{itemize}
\end{itemize}

\chapter{Spectral Approaches and The Inverse Covariance}

\begin{itemize}
\item From the {}``covariance interpretation'' we think of the similarity
matrix as a covariance.
\item Each element of the covariance is a function of two data points.
\item Another option is to specify the inverse covariance.


If the inverse covariance between two points is zero. Those points
are independent given all other points.
\begin{itemize}
\item This is a \emph{conditional independence}. 
\item Describes how points are connected.
\end{itemize}
\item Laplacian Eigenmaps and LLE can both be seen as specifiying the inverse
covariance.
\end{itemize}
LLE Examples

%
\footnote{7 neighbours used. No playing with settings.%
}

%
\begin{figure}
\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/planeData}}\hfill{}

\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/demLlePlane}}

\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/swissrollData}}\hfill{}\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/demLleSwissroll}}

\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/trefoilData}}\hfill{}\subfigure[]{

\includegraphics[width=0.5\textwidth]{../diagrams/demLleTrefoil}}\caption{Figure}

\end{figure}


\texttt{demLle}

\section{Generative}
\begin{itemize}
\item Observed data have been sampled from manifold 
\item Spectral methods start in the {}``wrong'' end 
\item \textit{{}``It's a lot easier to make a mess than to clean it up!''} 

\begin{itemize}
\item Things break or disapear 
\end{itemize}
\item How to model observation {}``generation''? 
\item Laplacian Eigenmaps \cite{Belkin:laplacian03}.

\begin{itemize}
\item Uses spectral graph theory and information geometric arguments to
form embedding.
\item Compute neighborhood, graph Laplacian and seek 2nd lowest eigenvector.
\end{itemize}
\end{itemize}


\section{Spring Analogy}

The next three approaches to spectral dimensionality reduction we consider can all be summarized through a simple analogy.


The energy of the spring system is given by 
\[
  E\left(\latentMatrix, \springMatrix\right) = \tr{\latentMatrix^\top \latentMatrix} + \sum_{i=1}^\numData \sum_{j=1}^\numData \springMatrix_{i,j} \left(\latentVector_{i,:} - \latentVector_{j, :}\right)^\top\left(\latentVector_{i,:} - \latentVector_{j, :}\right) - \left(\dataVector_{i, :} - \dataVector_{j,:}\right)^\top \left(\dataVector_{i, :} - \dataVector_{j,:}\right)
\]
We can solve this system by first considering that we can decompose 
\[
\latentMatrix = \eigenvectorMatrix_\latentDim \eigenvalueMatrix_\latentDim \rotationMatrix^\top
\]
where as before this is a single value decomposition of the latent
matrix and we have $\eigenvectorMatrix_\latentDim^\top
\eigenvectorMatrix_\latentDim = \eye$,
$\rotationMatrix^\top\rotationMatrix = \rotationMatrix
\rotationMatrix^\top = \eye$, and $\eigenvalueMatrix\in\Re^{\latentDim
  \times \latentDim}$ is diagonal.

This allows us to write the energy as
\[
  E\left(\latentMatrix, \springMatrix\right) = \sum_{j=1}^\latentDim \eigenvalue^2_j  + \sum_{i=1}^\numData \sum_{j=1}^\numData \springMatrix_{i,j} \left(\eigenvector_{i,:} - \eigenvector_{j, :}\right)^\top\eigenvalueMatrix^2\left(\eigenvector_{i,:} - \eigenvector_{j, :}\right) - \left(\dataVector_{i, :} - \dataVector_{j,:}\right)^\top \left(\dataVector_{i, :} - \dataVector_{j,:}\right).
\]
Let's minimize this with respect to one of the eigenvectors, $\eigenvector_{k, :}$. This requires that we introduce a constraint that $\eigenvector_{:,k}^\top\eigenvector_{:,k} = 1$. We can add this constraint through a Lagrange multiplier,
\[
  E\left(\latentMatrix, \springMatrix\right) = \sum_{j=1}^\latentDim \eigenvalue^2_j  + \sum_{i=1}^\numData \sum_{j=1}^\numData \springMatrix_{i,j} \left(\eigenvector_{i,:} - \eigenvector_{j, :}\right)^\top\eigenvalueMatrix^2\left(\eigenvector_{i,:} - \eigenvector_{j, :}\right) - \left(\dataVector_{i, :} - \dataVector_{j,:}\right)^\top \left(\dataVector_{i, :} - \dataVector_{j,:}\right) - \gamma_{k,k}\left(\eigenvector_{:, k}^\top\eigenvector_{:,k}-1\right).
\]

\todo{Need to tidy this up!}
We have purposefully labeled $\eigenvector_{1,:}$ as 1 to indicate it is the first to be extracted. Since it is the first to be extracted we can ignore the constraint that it should be normal to the other eigenvectors: this constraint is not needed for the first vector as it can be imposed on the. We think of it as the primary eigenvector: the first eigenvector to be extr (not necessarily the one with the largest eigenvalue). By extracting the eigenvectors one at a timeThis allows us to ignore the constraint that  but as there is also a constraint that this eigenvector Now we take gradients with respect to $\eigenvector_{1,:}$, 
Gradients with respect to the eigenvector can be found  as:
\[
\frac{\mathrm{d}E}{\mathrm{d}\eigenvector{:, 1}} = 2\laplacianMatrix \eigenvector_{:, 1}\eigenvalue^2_1  - 2\gamma_{1, 1} \eigenvector_{:,1}.
\]
We have defined the matrix $\laplacianMatrix$ to be a stiffness matrix, for which the $i$th diagonal element is given by $\laplacianScalar_{i, i}= \sum_{j=1}^\numData \left(\springScalar_{i, j} + \springScalar_{j, i}\right)$ and the off diagonal elements are given by $\laplacianScalar_{i, j} = -\left(\springScalar_{i, j} + \springScalar_{j, i}\right)$. The solution is therefore given by the following eigenvalue problem,
\[
 \laplacianMatrix \eigenvector_{:, 1}  = \frac{\gamma_{k, k}}{\eigenvalue^{-2}_1} \eigenvector_{1,:}.
\]
The eigenvector that will do the most to minimize the energy is the one with the largest $\eigenvalue^{2}$????

We can now proceed by extracting $\eigenvector_{:, 2}$, although we must add the additional constraint that it is orthogonal to $\eigenvector_{:, 1}$,
\[
\frac{\mathrm{d}E}{\mathrm{d}\eigenvector{:, 2}} = 2\laplacianMatrix \eigenvector_{:, 2}\eigenvalue^2_2  - 2\gamma_{2, 2} \eigenvector_{:,2}. - \gamma_{2,1}\eigenvector_{1, :}
\]
this allows us to write down the solution for $\eigenvectorMatrix_\latentDim$ as
\[
\laplacianMatrix \eigenvectorMatrix_\latentDim = \eigenvectorMatrix_\latentDim \eigenvalueMatrix_\latentDim^{-2} \Gamma
\]
where $\Gamma$ is a diagonal matrix whose diagonal elements will be $\gamma_{i,i}=\eigenvector_{:, i}^\top \laplacianMatrix \eigenvector_{:, i}\eigenvalue_{i}^{2}$. 

Only $\latentDim$ of the eigenvectors should be retained, \todo{Explain that the smallest are retained, but the very smallest is discarded as it reflects the translation invariance of the model}.

Inverse covariance and products of Gaussians models!


\section{Maximum Variance Unfolding}
Optimization of $\eigenvalue_{i}^2$ a

%\[
%E\left(\latentMatrix, \laplacianMatrix, \Gamma\right) = -\tr{\latentMatrix \latentMatrix^\top) - \gamm
%\]






of the are diagonal matrices
NEED TO SHOW $\gamma_{2, 1}$ is zero! 
which shows that
Further eigenvectors can be extracted by adding the constraint that they should be orthogonal to previously extracted eiten



\subsubsection{Mattress Model}
\begin{itemize}
\item Points are connected by springs.
\end{itemize}  
\begin{figure}
  \includegraphics[width=0.6\textwidth]{../diagrams/neighboursGridData-20}
  \caption{Physical interpretation of spectral models.}

\end{figure}



\subsubsection{Spring Energy}
\begin{itemize}
\item Points are connected by springs.
\item Each spring has its only spring constant.
\item Place each point at its latent location.
\item Potential energy in each string is given by.
\item Which can be expressed as a latent distance.
\item Energy associated with each point given by sum.
\item Energy associated with system is sum over points.
\end{itemize}
\begin{figure}
  \begin{comment}
  \begin{pgfpicture}{-0.1\textwidth}{-0.1\textheight}{0.7\textwidth}{0.7\textheight}
    \pgfsetxvec{\pgfpoint{0.6\textwidth}{0}}
    \pgfsetyvec{\pgfpoint{0}{0.6\textheight}}
    
    %%%%%%%%%%%%%%%% GUIDELINES %%%%%%%%%%%%%%%%%%%%%    
    % \pgfline{\pgfxy(0, 0)}{\pgfxy(0, 1)}
    % \pgfline{\pgfxy(0.1, 0)}{\pgfxy(0.1, 1)}
    % \pgfline{\pgfxy(0.2, 0)}{\pgfxy(0.2, 1)}
    % \pgfline{\pgfxy(0.3, 0)}{\pgfxy(0.3, 1)}
    % \pgfline{\pgfxy(0.4, 0)}{\pgfxy(0.4, 1)}
    % \pgfline{\pgfxy(0.5, 0)}{\pgfxy(0.5, 1)}
    % \pgfline{\pgfxy(0.6, 0)}{\pgfxy(0.6, 1)}
    % \pgfline{\pgfxy(0.7, 0)}{\pgfxy(0.7, 1)}
    % \pgfline{\pgfxy(0.8, 0)}{\pgfxy(0.8, 1)}
    % \pgfline{\pgfxy(0.9, 0)}{\pgfxy(0.9, 1)}
    % \pgfline{\pgfxy(1, 0)}{\pgfxy(1, 1)}

    % \pgfline{\pgfxy(0, 0)}{\pgfxy(1, 0)}
    % \pgfline{\pgfxy(0, 0.1)}{\pgfxy(1, 0.1)}
    % \pgfline{\pgfxy(0, 0.2)}{\pgfxy(1, 0.2)}
    % \pgfline{\pgfxy(0, 0.3)}{\pgfxy(1, 0.3)}
    % \pgfline{\pgfxy(0, 0.4)}{\pgfxy(1, 0.4)}
    % \pgfline{\pgfxy(0, 0.5)}{\pgfxy(1, 0.5)}
    % \pgfline{\pgfxy(0, 0.6)}{\pgfxy(1, 0.6)}
    % \pgfline{\pgfxy(0, 0.7)}{\pgfxy(1, 0.7)}
    % \pgfline{\pgfxy(0, 0.8)}{\pgfxy(1, 0.8)}
    % \pgfline{\pgfxy(0, 0.9)}{\pgfxy(1, 0.9)}
    % \pgfline{\pgfxy(0, 1)}{\pgfxy(1, 1)}
    %%%%%%%%%%%%%%%% GUIDELINES %%%%%%%%%%%%%%%%%%%%%    
    \pgfputat{\pgfxy(0,1)}{\pgfbox[left,base]{\includegraphics[width=0.6\textwidth]{../diagrams/springEnergy}}}
    \llap{
        \pgfputat{\pgfxy(0.615, 0.755)}{\pgfbox[left,base]{{$1$}}}
        \pgfputat{\pgfxy(0.325, 0.852)}{\pgfbox[left,base]{{$2$}}}
        \pgfputat{\pgfxy(0.605, 0.545)}{\pgfbox[left,base]{{$3$}}}
        \pgfputat{\pgfxy(0.945, 0.64)}{\pgfbox[left,base]{{$4$}}}
        \pgfputat{\pgfxy(0.322, 0.23)}{\pgfbox[left,base]{{$5$}}}
        \pgfputat{\pgfxy(0.295, 0.43)}{\pgfbox[left,base]{{$6$}}}
        \pgfputat{\pgfxy(0.03, 0.21)}{\pgfbox[left,base]{{$7$}}}
        \pgfputat{\pgfxy(0.364, 0.03)}{\pgfbox[left,base]{{$8$}}}
        \pgfputat{\pgfxy(0.66, 0.25)}{\pgfbox[left,base]{{$9$}}}
    }
    \llap{
        \pgfputat{\pgfxy(0.44, 0.72)}{\pgfbox[left,base]{{$\springScalar_{1,2}$}}}
        \pgfputat{\pgfxy(0.67, 0.62)}{\pgfbox[left,base]{{$\springScalar_{1,3}$}}}
        \pgfputat{\pgfxy(0.81, 0.6)}{\pgfbox[left,base]{{$\springScalar_{1,4}$}}}
        \pgfputat{\pgfxy(0.38, 0.38)}{\pgfbox[left,base]{{$\springScalar_{5,6}$}}}
        \pgfputat{\pgfxy(0.15, 0.135)}{\pgfbox[left,base]{{$\springScalar_{5,7}$}}}
        \pgfputat{\pgfxy(0.40, 0.13)}{\pgfbox[left,base]{{$\springScalar_{5,8}$}}}
        \pgfputat{\pgfxy(0.54, 0.16)}{\pgfbox[left,base]{{$\springScalar_{5,9}$}}}
    }
    \llap{
        \pgfputat{\pgfxy(0.605, 0.763)}{\pgfbox[left,base]{{\footnotesize $\latentVector_{1, :}$}}}
        \pgfputat{\pgfxy(0.315, 0.863)}{\pgfbox[left,base]{{\footnotesize $\latentVector_{2,:}$}}}
        \pgfputat{\pgfxy(0.595, 0.555)}{\pgfbox[left,base]{{\footnotesize $\latentVector_{3,:}$}}}
        \pgfputat{\pgfxy(0.935, 0.65)}{\pgfbox[left,base]{{\footnotesize $\latentVector_{4, :}$}}}
        \pgfputat{\pgfxy(0.312, 0.24)}{\pgfbox[left,base]{{\footnotesize $\latentVector_{5, :}$}}}
        \pgfputat{\pgfxy(0.285, 0.44)}{\pgfbox[left,base]{{\footnotesize $\latentVector_{6, :}$}}}
        \pgfputat{\pgfxy(0.02, 0.22)}{\pgfbox[left,base]{{\footnotesize $\latentVector_{7, :}$}}}
        \pgfputat{\pgfxy(0.355, 0.04)}{\pgfbox[left,base]{{\footnotesize $\latentVector_{8, :}$}}}
        \pgfputat{\pgfxy(0.65, 0.26)}{\pgfbox[left,base]{{\footnotesize $\latentVector_{9, :}$}}}
    }
    \llap{
        \pgfputat{\pgfxy(0.49, 0.9)}{\pgfbox[left,base]{{\small $ E_{1,2}=\springScalar_{1,2} (\latentVector_{1,:} -\latentVector_{2,:})^\top(\latentVector_{1,:} -\latentVector_{2,:})$}}}
    }
    \llap{
        \pgfputat{\pgfxy(0.49, 0.9)}{\pgfbox[left,base]{{\small $ E_{1,2}=\springScalar_{1,2} \latentDistanceScalar_{1,2}^2$}}}
    }
    \llap{
        \pgfputat{\pgfxy(-.1, 0.7)}{\pgfbox[left,base]{{\small $E_1(\latentMatrix) = \sum_{j}\springScalar_{1,j} \latentDistanceScalar^2_{1, j}$}}}
    }
    \llap{
        \pgfputat{\pgfxy(0.7, -0.1)}{\pgfbox[left,base]{{\small $E_5(\latentMatrix) = \sum_{j}\springScalar_{5,j} \latentDistanceScalar^2_{5, j}$}}}
    }

  \end{pgfpicture}
\end{comment}
\end{figure}


\subsubsection{Physical Analogy}
\begin{itemize}
\item System total energy given by
  $
  E(\latentMatrix, \springMatrix) = \sum_{i=1}^\numData \sum_{j=1}^\numData \kappa_{i,j} \latentDistanceScalar_{i,j}^2
  $
\end{itemize}
\begin{figure}
  \begin{comment}
  \begin{pgfpicture}{0\textwidth}{-0.1\textheight}{0.4\textwidth}{0.5\textheight}
    \pgfsetxvec{\pgfpoint{0.4\textwidth}{0}}
    \pgfsetyvec{\pgfpoint{0}{0.4\textheight}}

    \pgfputat{\pgfxy(0,1)}{\pgfbox[left,base]{\includegraphics[width=0.4\textwidth]{../diagrams/springEnergy}}}
    \llap{
        \pgfputat{\pgfxy(0.615, 0.755)}{\pgfbox[left,base]{{\scriptsize $1$}}}
        \pgfputat{\pgfxy(0.325, 0.852)}{\pgfbox[left,base]{{\scriptsize $2$}}}
        \pgfputat{\pgfxy(0.605, 0.545)}{\pgfbox[left,base]{{\scriptsize $3$}}}
        \pgfputat{\pgfxy(0.945, 0.64)}{\pgfbox[left,base]{{\scriptsize $4$}}}
        \pgfputat{\pgfxy(0.322, 0.23)}{\pgfbox[left,base]{{\scriptsize $5$}}}
        \pgfputat{\pgfxy(0.295, 0.43)}{\pgfbox[left,base]{{\scriptsize $6$}}}
        \pgfputat{\pgfxy(0.03, 0.21)}{\pgfbox[left,base]{{\scriptsize $7$}}}
        \pgfputat{\pgfxy(0.364, 0.03)}{\pgfbox[left,base]{{\scriptsize $8$}}}
        \pgfputat{\pgfxy(0.66, 0.25)}{\pgfbox[left,base]{{\scriptsize $9$}}}
    }
      {       \color{red}
        \pgfsetendarrow{\pgfarrowto}
        \newcommand{\startPoint}{\pgfpartway{1.35}{\pgfxy(0.5, 0.5)}{\pgfxy(0.615, 0.755)}}
        \newcommand{\finishPoint}{\pgfpartway{1.5}{\pgfxy(0.5, 0.5)}{\startPoint}}
        \pgfline{\startPoint}{\finishPoint}
        \renewcommand{\startPoint}{\pgfpartway{1.3}{\pgfxy(0.5, 0.5)}{\pgfxy(0.325, 0.852)}}
        \renewcommand{\finishPoint}{\pgfpartway{1.45}{\pgfxy(0.5, 0.5)}{\startPoint}}
        \pgfline{\startPoint}{\finishPoint}
        \renewcommand{\startPoint}{\pgfpartway{1.55}{\pgfxy(0.5, 0.5)}{\pgfxy(0.605, 0.545)}}
        \renewcommand{\finishPoint}{\pgfpartway{1.7}{\pgfxy(0.5, 0.5)}{\startPoint}}
        \pgfline{\startPoint}{\finishPoint}
        \renewcommand{\startPoint}{\pgfpartway{1.2}{\pgfxy(0.5, 0.5)}{\pgfxy(0.945, 0.64)}}
        \renewcommand{\finishPoint}{\pgfpartway{1.35}{\pgfxy(0.5, 0.5)}{\startPoint}}
        \pgfline{\startPoint}{\finishPoint}
        
        \renewcommand{\startPoint}{\pgfpartway{1.2}{\pgfxy(0.5, 0.5)}{\pgfxy(0.322, 0.23)}}
        \renewcommand{\finishPoint}{\pgfpartway{1.35}{\pgfxy(0.5, 0.5)}{\startPoint}}
        \pgfline{\startPoint}{\finishPoint}
        \renewcommand{\startPoint}{\pgfpartway{1.2}{\pgfxy(0.5, 0.5)}{\pgfxy(0.295, 0.43)}}
        \renewcommand{\finishPoint}{\pgfpartway{1.35}{\pgfxy(0.5, 0.5)}{\startPoint}}
        \pgfline{\startPoint}{\finishPoint}
        \renewcommand{\startPoint}{\pgfpartway{1.2}{\pgfxy(0.5, 0.5)}{\pgfxy(0.03, 0.21)}}
        \renewcommand{\finishPoint}{\pgfpartway{1.35}{\pgfxy(0.5, 0.5)}{\startPoint}}
        \pgfline{\startPoint}{\finishPoint}
        \renewcommand{\startPoint}{\pgfpartway{1.2}{\pgfxy(0.5, 0.5)}{\pgfxy(0.364, 0.03)}}
        \renewcommand{\finishPoint}{\pgfpartway{1.35}{\pgfxy(0.5, 0.5)}{\startPoint}}
        \pgfline{\startPoint}{\finishPoint}
        \renewcommand{\startPoint}{\pgfpartway{1.2}{\pgfxy(0.5, 0.5)}{\pgfxy(0.66, 0.25)}}
        \renewcommand{\finishPoint}{\pgfpartway{1.35}{\pgfxy(0.5, 0.5)}{\startPoint}}
        \pgfline{\startPoint}{\finishPoint}
    }
  \end{pgfpicture}
  \end{comment}
\end{figure}
\begin{itemize}
\item Include a force to repel points from the origin
  {\small \[
    E(\latentMatrix, \springMatrix) = {\color{red} -\sum_{i=1}^\numData \latentVector_{i,:}^\top \latentVector_{i,:}} + \sum_{i=1}^\numData \sum_{j=1}^\numData \kappa_{i,j} \latentDistanceScalar_{i,j}^2
    \]
  }
\end{itemize}




\subsubsection{Energy Minimization}

\begin{itemize}
\item Minimization with respect to $\latentMatrix$ gives the following eigenvalue problem
  \[
  \laplacianMatrix \eigenvectorMatrix = \eigenvectorMatrix \Gamma \eigenvalueMatrix^{-2}
  \]      
  where $\laplacianMatrix$ is the \href{http://en.wikipedia.org/wiki/Stiffness_matrix}{stiffness matrix} (which is also a Laplacian matrix) from the
  graph. 
  \begin{align*}
    \laplacianScalar_{i,i} = & \sum_{j=1}^\numData(\kappa_{i,j} + \kappa_{j,i}) \\
    \laplacianScalar_{i,j} = &-(\kappa_{j,i} +
    \kappa_{i,j})
  \end{align*}
  and 
  \[
  \latentMatrix = \eigenvectorMatrix
  \eigenvalueMatrix \rotationMatrix^\top
  \] 
  and eigenvectors associated with the smallest eigenvalues are retained.\footnote{Actually the smallest eigenvalue is zero and its eigenvector is discarded. This reflects the indeterminability of the mean.}
\end{itemize}



\subsubsection{Where Do the Spring Constants Come From?}
\begin{itemize}
\item Algorithms assume only neighbors in data space are connected by springs (sparse connectivity).
\item Different algorithms suggest different values for the springs.
  \begin{itemize}
  \item Laplacian Eigenmaps prescribe constant spring constants, or values from an RBF on the distances {\scriptsize \citep{Belkin:laplacian03}}.
  \item Locally Linear Embedding considers spring constants that lead to optimal linear reconstruction of data points {\scriptsize \citep{Roweis:lle00}}.
  \item Maximum Variance Unfolding prescribes spring constants that constrain interpoint latent distances to equal those in the data {\scriptsize \citep{Weinberger:learning04}}.
  \end{itemize}
\end{itemize}



\subsubsection{Laplacian Eigenmaps}
\begin{itemize}
\item Either: Assign spring constant as 1 if points are neighbors, zero otherwise.
\item Or: Assign spring constant as $\springScalar_{i,j} =
  \exp\left(-\frac{\distanceScalar_{i,j}}{2 \lengthScale^2}\right)$
  for a given $\lengthScale$.
  \begin{itemize}
  \item Inspired by Green's function for spatial diffusion of heat.
  \end{itemize}
\item For each point \emph{normalize} the sum of its spring
  constants to 1.
\end{itemize}  




\subsubsection{LE on Stick Man}
\begin{itemize}
\item Two components of stick man data.
\end{itemize}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{../diagrams/demStickLe1}
    \caption{Stick man data embedded using two dimensions of LE. \texttt{demStickLe1}.}
  \end{center}
\end{figure}



\subsubsection{LE on Grid Vowels}
\begin{itemize}
\item Two components of grid vowels data.
\end{itemize}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{../diagrams/demGrid_vowelsLe1}
    \caption{Grid vowels embedded using two dimensions of LE. \texttt{demGrid\_vowelsLe1}.}
  \end{center}
\end{figure}





\subsubsection{Maximum Variance Unfolding}
\begin{itemize}
\item Maximize all interpoint distances under a constraint:
  \begin{itemize}
  \item Constrain neighbors to have their distances preserved matched to data space.
  \end{itemize}
  
\item Find the spring constants that respect these distances.
\item Analogy: manifold is being pulled apart but rods hold it
  together between neighbors.
\end{itemize}




\subsubsection{MVU on Stick Man}
\begin{itemize}
\item Two components of stick man data.
\end{itemize}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{../diagrams/demStickMvu1}
    \caption{Stick man data embedded using two dimensions of isomap. \texttt{demStickMvu1}.}
  \end{center}
\end{figure}


\subsubsection{MVU on Oil Data}
\begin{itemize}
\item Graph doesn't fully connect until 30 neighbors are used.
\item Resulting semi-definite program is too big for SeDuMi on my machine (32GB memory, but it swaps in MATLAB). 
\item There is approximate version of the algorithm, not applied in this case.
\end{itemize}


\subsubsection{MVU on Grid Vowels}
\begin{itemize}
\item Two components of grid vowels data.
\end{itemize}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{../diagrams/demGrid_vowelsMvu1}
    \caption{Grid vowels embedded using two dimensions of
      isomap. \texttt{demGrid\_vowelsMvu1}.}
  \end{center}
\end{figure}



\subsubsection{Locally Linear Embedding}
\begin{itemize}
\item Try and find a low dimensional space where locally linear relationships are preserved.
\item For each neighborhood, find values that predict a data point given a weighted sum of neighbors:
  \[
  \dataVector_{i,:} = \sum_{j} \mappingScalar_{i, j} \dataVector_{j, :}
  \]
\item Constrain sum of weights to equal $-\mappingScalar_{i,i}=1$.
\item Set stiffness matrix, $\laplacianMatrix = \mappingMatrix\mappingMatrix^\top$.
\end{itemize}





\subsubsection{LLE on Stick Man}
\begin{itemize}
\item Two components of stick man data.
\end{itemize}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{../diagrams/demStickLle1}
    \caption{Stick man data embedded using two dimensions of
      LLE. \texttt{demStickLle1}.}
  \end{center}
\end{figure}



\subsubsection{LLE on Oil Data}

\begin{itemize}
\item Two components of oil data.
\end{itemize}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{../diagrams/demOilLle1}
    \caption{Oil data embedded using two dimensions of LLE (graph is
      disconnected). \texttt{demOilLle1}. 18 nearest neighbor
      classification errors in latent space.}
  \end{center}
\end{figure}



\subsubsection{LLE on Grid Vowels}

\begin{itemize}
\item Two components of grid vowels data.
\end{itemize}
\begin{figure}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{../diagrams/demGrid_vowelsLle1}
    \caption{Grid vowels embedded using two dimensions of
      LLE. \texttt{demGrid\_vowelsLle1}.}
  \end{center}
\end{figure}



\subsection{Specify the Inverse Covariance}



\subsubsection{Inverse Covariance}
\begin{itemize}
\item From the ``covariance interpretation'' we think of the
  similarity matrix as a covariance matrix.
  \begin{itemize}
  \item Each element of the covariance is a function of two data
    points.
  \end{itemize}
\item For LE, LLE and MVU the stiffness matrix is like an \emph{inverse covariance}.
  \begin{itemize}
  \item This is a \emph{conditional independence} assumption. 
  \item Describes how points are connected.
  \end{itemize}
\end{itemize}


If a probabilistic model is defined in terms of a Laplacian matrix, $\laplacianMatrix$. Then we can write down a Gaussian density as
\[
p\left(\dataMatrix\right) = \frac{\det{\laplacianMatrix + \dataStd^{-2}\eye}^{\frac{\dataDim}{2}}}{(2\pi)^{\frac{\numData\dataDim}{2}}} \exp\left(-\frac{1}{2}\tr{\left(\laplacianMatrix + \dataStd^{-2}\right)\dataMatrix \dataMatrix^\top}\right)
\]
Now we note that the squared distance matrix, $\distanceMatrix$, can be expressed as
\[
\distanceMatrix = \diag{\dataMatrix \dataMatrix^\top} \onesVector^\top - 2\dataMatrix \dataMatrix^\top + \onesVector \diag{\dataMatrix \dataMatrix^\top}
\]
which implies that 
\begin{align*}
\laplacianMatrix \distanceMatrix = - 2\laplacianMatrix \dataMatrix \dataMatrix^\top\\
-\frac{1}{2}\laplacianMatrix \distanceMatrix = \laplacianMatrix \dataMatrix \dataMatrix^\top
\end{align*}
because $\laplacianMatrix \onesVector = \left[\onesVector^\top \laplacianMatrix\right]^\top=\zerosVector$ so $\tr{\laplacianMatrix\diag{\dataMatrix \dataMatrix^\top} \onesVector^\top} = \tr{\laplacianMatrix\onesVector \diag{\dataMatrix \dataMatrix^\top}} = 0$.

This means we can write the log likelihood as 
\[
\log p\left(\dataMatrix\right) = \frac{\dataDim}{2}\log \det{\laplacianMatrix + \dataStd^{-2}\eye} + \frac{1}{4}\tr{\left(\laplacianMatrix + \dataStd^{-2}\right)\distanceMatrix} + \mathrm{const}.
\]
where $\mathrm{const}$ is terms that are constant in $\laplacianMatrix$.
Derivatives with respect to the Laplacian matrix lead to 
\[
\frac{\mathrm{d} \log p\left(\dataMatrix\right)}{\mathrm{d}\laplacianMatrix} = \frac{\dataDim}{2} \left[\laplacianMatrix + \dataStd^{-2}\right]^{-1} + \frac{1}{4}\distanceMatrix
\]
The gradient of the Laplacian with respect to a single spring, $\springScalar_{i,j}$ is given by 
\[
\frac{\mathrm{d}\laplacianMatrix}{\mathrm{d}\kappa_{i,j}}= B^{i,j}
\] 
where $B_{i,j}$ is a sparse matrix which is non zero only at $b_{i,i}=b_{j,j}=1$ and $b_{i,j}=b_{j,i}=-1$. Thus the gradient of the log likelihood with respect to $\kappa_{i,j}$ is given by
\[
\frac{\mathrm{d}\log p(\dataMatrix)}{\mathrm{d}\kappa_{i,j}} = \frac{\dataDim}{2}\left[\kernelScalar_{i,i} + \kernelScalar_{j, j} - 2 \kernelScalar_{i, j}\right] - \frac{1}{2}\distanceScalar^2_{i,j}
\]
so at the solution
\[
\left[\kernelScalar_{i,i} + \kernelScalar_{j, j} - 2 \kernelScalar_{i, j}\right] = \frac{1}{\dataDim}\distanceScalar^2_{i,j}
\]

Nonlinear dimensionality reduction allows us to summarize a high
dimensional data set through a set of lower dimensional variables. A
particular popular approach explored over the last decade has been
spectral dimensionality reduction \citep[see e.g.][]{Tenenbaum:isomap00,Roweis:lle00,Belkin:laplacian03,Weinberger:learning04}. Spectral
approaches to dimensionality reduction involve solving an eigenvalue
problem. Beyond this, on initial reading of works in this area it may
not be completely clear how the approaches are related. Whilst they
all use eigendecompositions of a matrix, some of them seek
eigenvectors associated with the smallest eigenvalues, whilst others
seek those associated with the largest. In this paper, we introduce a
new, probabilistic interpretation for these embeddings. Through the
probabilistic perspective we relate our model to the other popular
approaches for spectral dimensionality reduction. We will first
introduce the general perspective we take through a spring-based
finite element approximation.

\subsection{The Mattress Model}

When lecturing on dimensionality reduction, an often used example is
that of a piece of (two dimensional) paper is crumpled so that it
lives in a three dimensional space. In this analogy, the process of
dimensionality reduction is the flattening out of the three
dimensional crumpled paper to a two dimensional space. The analogy can
also be extended to consider data spaces of higher dimensionality,
$\dataDim$, and higher dimensional, $\latentDim$, pieces of
paper. However, for the moment let's consider a two dimensional sheet
of paper crumpled in a $\dataDim$-dimensional space. 

A finite element model of the paper could consist of a grid of points,
each point connected to its nearest 4 neighbors through a
spring,\footnote{For higher dimensional paper, more neighbors would be
  considered, for lower dimensional paper (a piece of string) two
  neighbors would be considered.} like an old fashioned mattress. We
denote the spring constant connecting point\footnote{The spring
  constant could be negative (indicating a repulsive spring).}  $i$ to
point $j$ by $\springScalar_{i,j}$. If two points are unconnected we
can take $\springScalar_{i,j}=0$. We represent all springs in a
matrix, $\springMatrix$. Such a grid representing paper folded in
three dimensions is shown in Figure~\ref{fig:neighboursGridData}.
\begin{figure}
  \includegraphics[width=0.4\textwidth]{../diagrams/neighboursGridLatentInterspeech.pdf}
  \caption{Finite element model of a piece of paper. The paper is
    divided into a grid of points. Each point is connected to its
    neighbors through springs.}\label{fig:neighboursGridData}
\end{figure}

When the paper is deformed in high dimensions, we denote the location
of the $i$th point from the grid by $\dataVector_{i, :}$. We represent
all points from the grid in a \emph{design matrix},
$\dataMatrix\in^{\numData\times\dataDim}$. Using Hooke's law, we can
compute the potential energy of the system for a particular deformed
configuration of the points, $\dataMatrix$,
\[
E(\dataMatrix, \springMatrix) = \frac{1}{2}\sum_{i,j}
\springScalar_{i,j}(\dataVector_{i,:} -
\dataVector_{j,;})^\top(\dataVector_{i,:} - \dataVector_{j,:})
\]
To model paper of uniform density the nonzero spring constants would
all be fixed to the same value. However, in our derivations we will
consider the general case where they can vary in value as it will
prove useful when we extend our analogy.

The potential energy is given by the sum of the squared displacement
of the springs, multiplied by the spring constants. We can rewrite
this energy in matrix form as
\[
E(\dataMatrix, \springMatrix) = \frac{1}{2}\tr{\springMatrix \distanceMatrix}
\]
where $\distanceMatrix$ is the matrix of squared distances in the
latent space, each element being defined as
$\distanceScalar_{i,j}=(\dataVector_{i,:} -
\dataVector_{j,:})^\top(\dataVector_{i,:} - \dataVector_{j,:})$. This implies
that the full matrix of interpoint distances can be written in matrix
form as 
\begin{align}
  \distanceMatrix = \onesVector\diag{\dataMatrix\dataMatrix^\top}-
  2\dataMatrix\dataMatrix^\top
  +\diag{\dataMatrix\dataMatrix^\top}\onesVector^\top,\label{eq:distanceDefinition}
\end{align}
Here the operator $\diag{\cdot}$ extracts the diagonal of a matrix as
a column vector.

Such spring systems are commonly represented in the form of a
\emph{stiffness matrix}, $\laplacianMatrix$. The elements of this
matrix are given by
\begin{align}
  \laplacianScalar_{i,j}=-\frac{1}{2}(\springScalar_{i,j}+\springScalar_{j,
  i}) \,\,\mathrm{if}\,\,i\neq j \label{eqn:diagonalLaplacian}\\
\laplacianScalar_{i,i}=\frac{1}{2}\sum_{j\neq
  i} (\springScalar_{i,j} + \springScalar_{j,i}). \label{eqn:offdiagonalLaplacian}
\end{align}
The form of this matrix is recognized as a \emph{Laplacian}. As the
squared distance matrix is symmetric with diagonal elements set to
zero we can substitute the $\springMatrix$ directly with the negative
Laplacian form in our expression of the potential energy,
\begin{align*}
  E(\dataMatrix, \springMatrix) =& -\frac{1}{2}\tr{\laplacianMatrix\distanceMatrix}\\
  =&-\frac{1}{2}\tr{\laplacianMatrix\onesVector\diag{\dataMatrix\dataMatrix^\top}}+ \frac{1}{2}\tr{\laplacianMatrix \dataMatrix\dataMatrix^\top}\\& -\tr{\laplacianMatrix \diag{\dataMatrix\dataMatrix^\top}\onesVector^\top}\\
  =&\tr{\dataMatrix\dataMatrix^\top\laplacianMatrix }
\end{align*}
where the last line follows from the fact that the stiffness matrix
has a zero eigenvalue associated with the constant eigenvector. This
ensures that the first and last terms in the second line have zero
contribution. This effect is achieved by construction---each diagonal
element in the stiffness matrix is set to be the negative sum of the
corresponding row/column.

We are interested in a probabilistic interpretation of this model. A
probability density can be formed by exponentiating half the negative
energy and normalizing,
\[
p(\dataMatrix) \propto \exp\left(-\frac{1}{2}\tr{\laplacianMatrix
    \dataMatrix\dataMatrix^\top}\right)
\]
which implies a probabilistic model that is a \emph{Gaussian random field}. Note that the field is expressing the correlation between data points (not between data features). The full probabilistic model is arrived at by applying the model independently for each of the $\dataDim$ dimensions of $\dataMatrix$,
\[
p(\dataMatrix|\springMatrix) \propto
\exp\left(-\frac{1}{2}\sum_{j=1}^\dataDim\dataVector_{:,j}^\top\laplacianMatrix
  \dataVector_{:,j}\right).
\]
Unfortunately, in its current form our energy leads to a
unnormalizable model. The zero eigenvalue of the stiffness matrix
ensures that the model is insensitive to offsets independently applied
to the columns of $\dataMatrix$. In other words, it is unaffected by
changes to the mean of the grid points. This makes sense, as we
originally expressed the energy only in the form of interpoint
distances, which are unaffected by the data mean. From a probabilistic
point of view, a similar affect can be achieved by introducing a mean
value in the Gaussian random field and marginalizing it using an
uninformative (improper) prior. So we can think of our model as having
arisen from a Gaussian random field where the means have been
marginalized through a zero mean Gaussian prior with variance
approaching infinity. We retrospectively fix this by adding a constant
term to the $\laplacianMatrix$,
\[
p(\dataMatrix|\springMatrix, \gamma) \propto \lim_{\gamma\rightarrow 0}\exp\left(-\frac{1}{2}\tr{\left(\laplacianMatrix +
        \frac{1}{\gamma\numData}\onesVector\onesVector^\top\right)\dataMatrix\dataMatrix^\top}\right)
\]
which implies the density for the data is Gaussian with an inverse
covariance, $\kernelMatrix^{-1}$ (also known as the precision or
\emph{information matrix}), given by
\begin{align}
\kernelMatrix^{-1}=\laplacianMatrix +
\frac{1}{\gamma\numData}\onesVector\onesVector^\top.\label{eqn:covDefinition}
\end{align}

\subsection{Classical Scaling and the Covariance Matrix}

The covariance matrix, $\kernelMatrix$, is an important characteristic
of this system. From the covariance matrix we can directly compute the
expected matrix of interpoint squared distances,
\[
\latentDistanceMatrix=\expDist{\distanceMatrix}{p(\dataMatrix)} =
\onesVector \diag{\kernelMatrix}^\top -2\kernelMatrix +
\diag{\kernelMatrix}\onesVector^\top,
\]
Classical multidimensional scaling \citep[see][for
a very solid treatment]{Mardia:multivariate79} is a well known
approach to dimensionality reduction from statistics. It attempts to
visualize distances implied by a \emph{similarity matrix} in a
$\latentDim$-dimensional Euclidean space by extracting the first
$\latentDim$ principal eigenvectors\footnote{By principal eigenvectors
  we mean those associated with the largest eigenvalues.} of the
similarity matrix. For interpretation purposes we prefer to consider
the case where the similarity matrix is a covariance matrix. In our
case the covariance is expressing a Gaussian probabilistic
relationship between the data points. This allows the similarity
matrix to have negative entries. The proofs given in
\citet{Mardia:multivariate79} show how extracting the eigenvectors
associated with the largest $\latentDim$ eigenvalues of this
covariance matrix gives a lower dimensional configuration of points
in which the interpoint distances minimize the sum of absolute values
of the differences (i.e. the L1 error) between the expected square
distances given by the model,
$\expDist{\distanceMatrix}{p(\dataMatrix)}$ and a $\latentDim$
dimensional representation of the points. 

As an aside, we note that the \emph{kernel PCA} of
\citet{Scholkopf:nonlinear98} can be seen as an interesting special
case of classical scaling where the covariance matrix is expressed
directly as a function of the observed data through a Mercer
kernel\footnote{The use of a Mercer kernel to provide the covariance
  matrix also means that we obtain a mapping from the data space to
  the latent space as a side effect.} (which \emph{must} be a valid
covariance matrix as it is positive definite). The corresponding
expected distance between points in kernel PCA is computed in a
pairwise fashion, i.e., the interpoint distances for two points are
only dependent on those two points:
$\expDist{\distanceScalar_{i,j}}{p(\dataMatrix)} =
\kernelScalar(\dataVector_{i, :}, \dataVector_{i, :}) -
2\kernelScalar(\dataVector_{i, :}, \dataVector_{j, :}) +
\kernelScalar(\dataVector_{j, :}, \dataVector_{j, :})$, where here
$\kernelScalar(\cdot,\cdot)$ is the Mercer kernel.  As we will see,
for the spectral methods recently developed in the machine learning
community, the distance between two points is dependent on the entire
data set. In traditional classical scaling, a pairwise computation of
the distances is also made \citep[see, e.g.,][for a comprehensive
treatment of different distance measures]{Cox:bookxx}. In the spring
network model something else is going on. We are specifying the
\emph{inverse covariance} in a pairwise manner (two data are connected
by a spring if they are neighbors). What does this mean for the
covariance matrix?  This is easy to understand from a \emph{walk-sum}
point of view \citep{Malioutov:walksum06}.\footnote{One analogy that
  has been drawn to reflect this is that of diffusion maps
  \citep{Coifman:diffusion06} where diffusion occurs in the form of
  random walks across the discrete graph. This is a useful analogy,
  but due to limited space we will mainly focus on the walk-sum
  interpretation \citep{Malioutov:walksum06} of the Gaussian
  covariance that is more suited to our discussion.} The walk sum
algorithm is helpful in understanding the inverse of a sparse
information matrix. The basic result is that for a normalized
stiffness matrix (which would also be a normalized Laplacian matrix)
the correlation coefficient between two points is determined by the
sum of the length of all possible ``walks'' across the graph between
the two vertices associated with each point. The length of an
individual walk is given by the product of all the \emph{partial
  correlation} coefficient values
($\laplacianScalar_{i,j}/\sqrt{\laplacianScalar_{i,i}\laplacianScalar_{j,j}}$)
encountered on the walk. Since this value is constrained to be less
than one, longer walks typically contribute less to the computed
element of the covariance matrix than shorter walks.\footnote{Although
  the actual contribution will also depend on the geometric mean of
  the partial correlation coefficients encountered on the walk.} In
other words two points connected by short walks will typically have an
associated correlation coefficient, $k_{i,j}/\sqrt{k_{i,i}k_{j,j}}$,
that is dominated by those short walks which is likely to be
relatively large. This, in turn, will typically lead to a short
expected distance between the points,
$\expDist{\distanceScalar_{i,j}}{p(\dataMatrix)}$. Conversely, two
points only connected by long walks are likely to have a relatively
small associated correlation coefficient and a correspondingly larger
distance between the two points. Importantly, the interpoint distances
will be dependent on the \emph{entire data set}, as they all depend on
walks through the other data points. This contrasts with the situation
in kernel PCA and standard distance measures for classical
multidimensional scaling, where the distance between two points is
only dependent on those two points (other data is irrelevant).
\begin{figure*}
  \subfigure[stiffness matrix, $\laplacianMatrix$]{\includegraphics[width=0.23\textwidth]{../diagrams/Limage}}
  \hfill%$\rightarrow$\hfill
  \subfigure[covariance, $\kernelMatrix$]{\includegraphics[width=0.23\textwidth]{../diagrams/Kimage}}
  \hfill
  \subfigure[$\expDist{\distanceMatrix}{p(\dataMatrix)}$]{\includegraphics[width=0.23\textwidth]{../diagrams/D2image}}
  \hfill%$\rightarrow$\hfill
  \subfigure  [eigenvectors of $\laplacianMatrix$]{\includegraphics[width=0.23\textwidth]{../diagrams/latentPoints}}
  \caption{This figure shows the process we have described. For this
    example there are 36 points which we assume are laid out on a
    $6\times6$ grid. The stiffness matrix (a) is derived by assuming
    connecting points to their neighbors in the northerly, southerly,
    easterly, and westerly directions with springs, each with
    $\springScalar_{i,j}=1$. The corresponding covariance matrix is
    given in (b). The expected squared distances for points sampled
    from a Gaussian this covariance are given in (c), and the second
    and third smallest eigenvectors of $\laplacianMatrix$ (which are
    the 2nd and 3rd largest eigenvectors of the covariance) are
    plotted in (d), with lines showing points that are connected by
    springs. These coordinates lead to the optimal two dimensional
    reconstruction of the expected squared distances under an L1
    error. Note that in (d) points are not all equidistant from one
    another due to edge effects (points at the edge have fewer
    neighbors than those in the middle).}
\end{figure*}

In this paper we are suggesting that the key component in the
dimensionality reduction algorithm is the \emph{covariance matrix}: it
implies a certain expected interpoint distance between the data which
is then represented in a low dimensional configuration. Note that
other authors have tried to address directly what these distances are
by appealing to diffusion processes and the energy of the associated
spring system \citep{Coifman:diffusion06,Hughes:spring09}. These
perspectives are also useful but they differ from ours which is
attempting to see these dimensionality reductions as Gaussian random
fields. The physical interpretation of the field is only of
consequence in that it affects the covariance matrix of the field.

\subsection{Smallest Eigenvalues}

Note that it is not always necessary to invert the stiffness matrix and
extract the eigenvectors associated with the largest eigenvalues
directly in the manner of classical scaling. We can extract the
eigenvectors associated with the smallest eigenvalues directly from
the stiffness matrix: the eigenvectors of an inverse are the same, the
associated eigenvalues are simply found by inversion (thus we now need
to seek the smallest). Given that the stiffness matrix is sparse, this
approach is much more efficient. Both LLE and Laplacian eigenmaps take
advantage of this fact.

%%%POSSIBLE DEMO OF THIS SYSTEM? MENTION EIGENVECTORS ALREADY? HOW TO INTRODUCE MDS? 10x10 grid with stiffness, covariance and distance matrix, along with visualization of points. 

\section{Choice of Spring Constants}

For a uniformly stiff piece of paper, and a uniform grid of connected
points, setting the spring constants to a fixed value would be a
sensible model. Laplacian eigenmaps \citep{Belkin:laplacian03} suggest
this choice as one possibility when applying these models to
data. However, in practice, real data is unlikely to have been laid
out in a grid, so how do we apply this model to a realistic data set?
Let's refine our notation some more. The matrix, $\dataMatrix\in
\Re^{\numData\times \dataDim}$, now represents our data, which is no
longer constrained to lie on a grid in the latent space. We assume
that the data's intrinsic dimensionality is given by
$\latentDim<\dataDim$, so we can consider a lower dimensional
representation of the data given by $\latentMatrix$. It is in this low
dimensional space that we are interested in the neighborhood
relationships. So far we have assumed that the latent space was well
approximated by a finite element model, and we used the gridded
``data'' as nodes/vertices in the graph associated with this
model. When data doesn't lie on the grid, can we continue to apply our
finite element model? Firstly, if we knew distances between the data
points in the low dimensional space we could certainly encode the
neighborhood relationships: the nearest four neighbors of a data point
would be found directly from these distances. Given the neighbors, we
justified setting all the spring constants in the model to the same
value: it implied a uniform stiffness across our paper. However, when
the distances between neighbors are no longer uniform it doesn't make
sense to set the spring constants equally. If two data points are
further apart, we might expect that the spring connecting them would
be weaker. It turns out that the main differences between many of the
most popular spectral approaches to dimensionality reduction are in
how these spring constants are set. 

We introduce an algorithm in which the spring constants are found by a
maximum likelihood fit to the data. First, though, however, we have to
deal with another problem. Interpoint distances in the latent space
are \emph{not} known, so it is not trivial to identify the
neighborhood relations in this space.

It is common in dimensionality reduction to enforce the constraint
that in the low dimensional representation, $\latentMatrix$, two
points should be placed ``close together'' if they are also close in
the data space. This implies that there is be a mapping from the data
space to the latent space. However, the imposition of the existence of
such a mapping can be restrictive. It is quite possible for data to be
generated such that this mapping doesn't exist. For example, consider
a tangled kite string: it represents a one dimensional latent space
observed in three dimensions. At many points the string will cross
over itself. At these crossovers, data points sampled from the string
will be close together in $\dataMatrix$, but could be very distant in
$\latentMatrix$. A less restrictive assumption is that points which
are close in latent space should be forced to be close together in
data space. So, if two points are from close together on the string
(or piece of paper) they \emph{will} be close in data space. However,
due to the paper or string folding over on itself points might also be
close in data space even when they are actually distant in latent
space. This is a generative approach to dimensionality reduction: we
assume the data are sampled from points in the low dimensional space
and then they are projected to the high dimensional space by some
(potentially non-bijective) mapping \citep{MacKay:wondsa95,Bishop:gtm_ncomp98,Lawrence:pnpca05}.

A practical problem with the generative approach is that estimation of
distances in the latent space is non-trivial, whereas an estimation of
the interpoint distances in the data space is
straightforward. However, if we are prepared to countenance only
\emph{bijective mappings} from the latent data to the observed data, then we
can use the following approximation: if the mapping is bijective, then
local distances will be preserved in \emph{both} directions. We could
then use the local distances in data space as a \emph{proxy} for the
local distances in latent space. This may be sufficient for determining the
neighborhood relations.

In the next section we will combine the maximum likelihood approach
for optimizing spring constants in combination with the
``proxy-distance'' assumption for determining the neighbors to recover
an algorithm for dimensionality reduction.

\section{Maximum Likelihood for the Mattress Model}

Given a sparsity pattern for the connectivity of the springs as
determined by the neighbors in data space we have the sparsity pattern
for the stiffness matrix, $\laplacianMatrix$. Our probabilistic model
for the data then takes the form:
\[
p(\dataMatrix) = \prod_{j=1}^\dataDim \frac{\det{\laplacianMatrix +
    \gamma
    \numData^{-1}\onesVector\onesVector^\top}^{\frac{1}{2}}}{(2\pi)^{\numData/2}}\exp\left(-\frac{1}{2}\dataVector_{:,
    j}^\top\left(\laplacianMatrix + \gamma
    \numData^{-1}\onesVector\onesVector^\top\right)\dataVector_{:,
    j}\right).
\]were we assume any mean values for the data have been integrated as in
Appendix \ref{sec:improperMean}.

The log likelihood of the data is given by,
\begin{align*}
\log p(\dataMatrix) = &\frac{\dataDim}{2}\log \det{\laplacianMatrix +
  \gamma \numData^{-1}\onesVector\onesVector^\top}\\ 
&- \frac{1}{2}
\tr{\left(\laplacianMatrix + \gamma
    \numData^{-1}\onesVector\onesVector^\top\right)\dataMatrix
  \dataMatrix^\top}
\end{align*}
where we have replaced the sum over the data dimensions with in the
quadratic form with a trace. This allows us, in turn to write,
\begin{align*}
\log p(\dataMatrix) = &\frac{\dataDim}{2}\log \det{\laplacianMatrix +
  \frac{\gamma}{\numData} \onesVector\onesVector^\top} \\
&- \frac{1}{2}
\tr{\laplacianMatrix\dataMatrix\dataMatrix^\top}
-\frac{\gamma}{2\numData}\onesVector^\top\dataMatrix
\dataMatrix^\top\onesVector
\end{align*}
where we have used the fact that the sum over the matrix inner
products is equal to the trace operation (the sum over diagonal
elements), and that the trace of a sum is the sum of the traces.

Now consider trace of the matrix product
$\tr{\laplacianMatrix\dataMatrix\dataMatrix^\top}$. Since
$\laplacianMatrix$ has a null space in the constant vector we can use
the definition of the squared distance matrix, $\distanceMatrix$,
given in (\ref{eq:distanceDefinition}), and the commutation properties
of matrices within a trace to recover
$\tr{\laplacianMatrix\dataMatrix\dataMatrix^\top} =
-\frac{1}{2}\tr{\laplacianMatrix\distanceMatrix}$. Note also that
$\dataMatrix^\top\onesVector=\sum_{i=1}^\numData \dataVector_{i,
  :}=\numData\bar{\dataVector}$ is $\numData$ times the mean of the
data matrix. We then seen the log-likelihood can be expressed in terms
of the mean of the data matrix and the interpoint distances defined by
the data matrix.
\[
\log p(\dataMatrix) = \frac{\dataDim}{2}\log \det{\laplacianMatrix +
  \gamma/\numData \onesVector\onesVector^\top} + \frac{1}{4}
\tr{\laplacianMatrix\distanceMatrix}
-\frac{\numData}{2}\gamma\bar{\dataVector}^\top\bar{\dataVector}.
\]

Gradients of the log likelihood with respect to the stiffness matrix
can be computed as
\[
\frac{\mathrm{d}\log p(\dataMatrix)}{\mathrm{d}\laplacianMatrix} =
\frac{\dataDim}{2}\left(\laplacianMatrix +
  \frac{\gamma}{\numData}\onesVector\onesVector^\top\right)^{-1} +
\frac{1}{4} \distanceMatrix.
\]
Using the definition of $\kernelMatrix$ in (\ref{eqn:covDefinition})
we can write
\[
\frac{\mathrm{d}\log p(\dataMatrix)}{\mathrm{d}\laplacianScalar_{i,j}}
= \frac{\dataDim}{2}\kernelScalar_{i,j} + \frac{1}{4}
\distanceScalar_{i,j}.
\]
Using the definition in (\ref{eqn:diagonalLaplacian}) and
(\ref{eqn:offdiagonalLaplacian}) we can compute the corresponding
gradients with respect to the spring constants using the chain rule,
% , and
% we find,
% \begin{align*}
%   \frac{\mathrm{d}\laplacianScalar_{i,j}}{\mathrm{d}\springScalar_{r,s}} &= 1 \mathrm{if} i==j \mathrm{and} i==r \mathrm{or} i==s\\
%   \frac{\mathrm{d}\laplacianScalar_{i,j}}{\mathrm{d}\springScalar_{r,s}}
%   &= -1 \mathrm{if} i\neq j \mathrm{and} (i==r \mathrm{and} j==s)
%   \mathrm{or} (i==s \mathrm{and} j==r)
% \end{align*}
% using the chain rule we recover,
\begin{align*}
  \frac{\mathrm{d}\log p(\dataMatrix)}{\mathrm{d}\springScalar_{i,j}} &= \frac{\dataDim}{2}\left(\kernelScalar_{i,i} - 2\kernelScalar_{i,j} + \kernelScalar_{j,j}\right) - \frac{1}{2} \distanceScalar_{i,j}\\
%  \frac{\mathrm{d}\log p(\dataMatrix)}{\mathrm{d}\springScalar_{i,j}}
 & =
  \frac{\dataDim}{2}\expDist{\distanceScalar_{i,j}}{p\left(\dataMatrix\right)}
  - \frac{1}{2} \distanceScalar_{i,j}
\end{align*}
so we can see that the gradient of the log likelihood with respect to
a spring is zero if the expected squared distance under our Gaussian
model is equal to the actual distance between the points. This is a neat result. It says that if two points are closer under the expected distance than the true distance the corresponding spring constant should be decreased. Conversely, if they are further under the expected distance than the true distance, the spring constant should be increased. Convergence only occurs when all distances match.

In practice, to ensure that the covariance stays positive definite, we
constrain all springs to be attractive (positive).\footnote{This constraint could
  be relaxed, but we would still need to ensure that the resulting
  Laplacian stays non-negative definite.} This means that the solution
would also be found when two data are closer under their expected
distance than their observed distance and the associated spring
constant is zero.

\subsection{Maximum Variance Unfolding}

In the maximum variance unfolding algorithm, a covariance
matrix\footnote{It is also referred to as a kernel matrix, however we
  prefer the term covariance matrix as kernel matrix implies that the
  associated mapping is easy to recover, but in this case it isn't} is
estimated through a semidefinite program. This covariance matrix is
optimized through maximizing the trace (equivalent to maximizing all
interpoint distances in the data) under the constraints that the
matrix is positive definite and that the expected interpoint squared
distances implied by the covariance matrix matches the observed
interpoint distances for points that are defined as neighbors. The
latter constraint is imposed through Lagrange multiplies. Up to a
constant (which does not affect the optimum), the gradient of the
objective with respect to these Lagrange multipliers is identical to
the gradient of the log likelihood with respect to the spring
constants. In other words the Lagrange multipliers can be seen as
springs. The positive definite constraint is also necessary for our
model, however, by constraining all springs to be positive the network
is considered ``attractive'' and the covariance will be positive
definite (given the constant term we added). This is equivalent to the
variant of MVU where the constraints are allowed to be violated by
having distances smaller than the observed distance.

The key difference is that in MVU the trace is maximized. This has the
affect of maximizing the interpoint distances for all data points that
aren't connected by springs. We could also introduce this effect to
our algorithm by introducing repulsive springs for all non-neighbors
in the graph. Care would have to be taken, though, to ensure that the
resulting algorithm has a positive definite covariance matrix. It may
well be that MVU is finding the optimal likelihood of just such a
model.

\subsection{Laplacian Eigenmaps}

In Laplacian eigenmaps \citep{Belkin:laplacian03} the suggested
setting for the spring constants is either a fixed value, or,
\[
\springScalar_{i,j} = \exp\left(-\frac{1}{\ell} d_{i,j}\right)
\]
where a scale parameter, $\ell$, is chosen. The Laplacian is also
normalized so that the diagonal is constant and set to one. This
allows us to interpret the inverse matrix as the result of a random
walk across the graph. It is this random walk that results in
comparisons with diffusion \citep{Coifman:diffusion06}, as the random
walk can be seen as a diffusion across a discrete graph.

\subsection{Locally Linear Embeddings}

In the locally linear embedding, weights are set according to an
optimal reconstruction of each point by its neighbors. This
reconstruction considers each point and its neighbors
independently. The spring weights are set according to these
reconstruction weights. In LLE normalization of the resulting
Laplacian is also suggested, so again an interpretation from a random
walk perspective is possible.

\subsection{Isomap}

In isomap the interpoint distance matrix is represented
directly. Distances between points are given by the summing individual
distances along edges of the graph to find the shortest distance
between two points. We have already discussed how inverting the
Laplacian gives a distance derived from the sum of all the walks
between two points in the graph. If the shortest possible walk between
two points is long, the walk sum becomes well approximated by the
shortest walk. This is a saddle point approximation to the 
path integral. It seems that isomap is exploiting
this saddle point approximation to derive the distances. A disadvantage of
the approach is that the approximation is not guaranteed to provide a
set of distances that can be embedded in a Euclidean space.

\section{Experiments}

We believe it is important, when evaluating spectral decomposition
methods, to apply them to real data sets. Whilst Swiss rolls and
spirals are very useful for illustrating a particular characteristic
of an algorithm (particularly when these methods were first
published), they are a poor proxy for realistic data sets. In
particular, these toy problems are typically very densely sampled in a
uniform manner and live in a relatively low ``high'' dimensional space
(typically three). We consider two real world data sets. For both data
sets we used 7 nearest neighbors to form the stiffness matrices. Code
to recreate all our experiments is in the supplementary
material. The first data set is a human motion capture example.

\subsection{Motion Capture Data}

The data consists of a 3-dimensional point cloud of the location of 34
points from a subject performing a run. This leads to a 102
dimensional data set containing 55 frames of motion capture. The
subject begins the motion from stationary and takes approximately
three strides of run. The data was made available by Ohio State
University. From a machine learning perspective this data is
characterized by a cyclic pattern during the strides of run. However,
the angle of run does change so there are slight differences for each
cycle. The data is very low noise, as the motion capture rig is
designed to extract the point locations of the subject to a high
precision.

We applied each of the spectral methods mentioned above to this
data. The resulting visualizations of the two dominant eigenvectors
(excluding the constant eigenvector if present) of the models' implied
covariance matrices are shown in Figure \ref{fig:embedStick}.
\begin{figure*}
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demStickLe1}}
  \hfill
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demStickLle1}}
  \hfill
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demStickIsomap1}}
  \hfill
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demStickMvu1}}
  \hfill
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demStickMeu1}}
  \hfill
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demStickDrill1}}

  \caption{(a) Laplacian eigenmaps, (b) Locally linear embedding, (c) isomap, (d) maximum variance unfolding, (e) the maximum entropy unfolding, (f) the DRILL embedding of the motion capture data. Models capture either the cyclic structure or the structure associated with the start of the run.}\label{fig:embedStick}
\end{figure*}


\subsection{Robot Navigation Example}

The second data set we use is a series of recordings from a robot as
it traces a square path in a building. The robot records the strength
of WiFi signals in an attempt to localize its position \citep[see][for
an application]{Ferris:wifi07}. Since the robot moves only in two
dimensions, the inherent dimensionality of the data should be two: the
reduced dimensional space should reflect the robots moves. The WiFi
signals are noisier than the motion capture data, so it makes an
interesting comparison for the two methods. The robot completes a
single circuit after entering from a separate corridor, so it is
expected to exhibit ``loop closure'' in the resulting map. The data
consists of 215 frames of measurement, each frame consists of the WiFi
signal strength of 30 access points.

The results for the range of spectral approaches are shown in Figure
\ref{fig:embedRobot}.
\begin{figure}
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demRobotWirelessLe1}}
  \hfill
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demRobotWirelessLle1}}
  \hfill
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demRobotWirelessIsomap1}}
  \hfill
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demRobotWirelessMvu1}}
  \hfill
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demRobotWirelessMeu1}}
  \hfill
  \subfigure[]{\includegraphics[width=0.30\textwidth]{../../../meu/tex/diagrams/demRobotWirelessDrill1}}

\caption{(a) Laplacian eigenmaps, (b) Locally linear embedding, (c) isomap, (d) maximum variance unfolding, (e) the maximum entropy unfolding and (f) the DRILL embedding of robot WiFi data. Some models struggle to captured the loop structure (perhaps because of the higher level of noise). Most models also show the noise present in the data WiFi signals. The proposed probabilistic approach does a nice job of smoothing away the noise, giving a truer reflection of the robots actual movements.}\label{fig:embedRobot}
\end{figure}
Here it may be that we have some advantage in the probabilistic
approach which appears less sensitive to the noise in the data than
the other spectral approaches.

\section{Overview}

We have introduced a probabilistic interpretation for spectral approaches for dimensionality reduction. By seeing them as a Gaussian random field based on a probabilistic spring model: equivalent to a sprung mattress placed in a heat bath, we developed a likelihood associated with these models. Maximization of the likelihood with respect to the spring constants leads to a sensible algorithm that matches expected squared distances under the model to observed squared distances. 
We have related our approach to other spectral techniques. Now we also note the relation to generative approaches to dimensionality reduction. The GP-LVM \citep{Lawrence:pnpca05} suggests applying a Gaussian process defined on the latent space independently across the observed features. Our model defined a Gaussian random field independently across the observed features. The advantage of the Gaussian process approach is that it actually defines a generative mapping from a continuous latent space to the observed data. The advantage of the approach proposed here is that, through the proxy-distance assumption, it shares the common feature of other spectral methods that the spectral decomposition to obtain the embedding is a convex problem. 



%%% Local Variables:
%%% TeX-master: "book"
%%% End:
