%{
\begin{octave}
  %}
  % Comment/MATLAB set up code
  importTool('dimred')
  dimredToolboxes
  randn('seed', 1e6)
  rand('seed', 1e6)
  if ~isoctave
    colordef white
  end
  patchColor = [0.8 0.8 0.8];
  colorFigs = false;
  % Text width in cm.
  textWidth = 13;
%{
%Start of Comment/MATLAB brackets
\end{octave}
\newglossaryentry{basis}{name={basis}, description={}}
\newglossaryentry{local_basis}{name={local basis}, description={}}
\newglossaryentry{radial_basis_function}{name={radial basis function}, description={}}
\newglossaryentry{log_normal}{name={log normal}, description={The log normal is a probability density defined over positive values, which is equivalent to defining a Gaussian (normal) density over the logarithm of the data.}}
\newglossaryentry{multilayer_perceptron}{name={multi-layer perceptron}, description={Multi-layer perceptrons are a form of mapping between two spaces that are inspired by models of the brain. From our perspective they can be seen as an adaptive basis function model, where the function is given by $\mappingFunction(\latentVector_{i, :}) = \mappingVector^\top \basisFunctionVector$ where the basis function themselves have parameters and depend on $\latentVector_{i, :}$ in the following way: $\basisFunction_k(\latentVector_{i, :}; \mappingVectorTwo_{k, :}) = \frac{1}{1+\exp(-\mappingVectorTwo_{k, :}^\top\latentVector_{i, :})}$.}}

\chapter{Nonlinear Probabilistic Dimensionality Reduction}\label{chap:nonlinear}

In \refchap{chap:linear} we showed how linear dimensionality reduction models can be constructed using a probabilistic model, where our reduced dimensional representation, $\latentMatrix$, is take to be \emph{latent variables}\index{latent variable}. We assumed a linear relationship between these latent variables and our data,
\[
\dataScalar_{i, j} = \mappingVector_{j, :}^\top \latentVector_{i, :} + \meanVector + \noiseScalar_{i, j},
\]
implying a Gaussian likelihood of the form
\[
p(\dataVector_{i, :}| \mappingMatrix, \latentVector_{i, :}, \dataStd^2) = \gaussianDist{\dataVector_{i, :}}{\mappingMatrix\latentVector_{i, :}}{\dataStd^2\eye}
\]
which can be written in matrix form as
\[
\dataMatrix = \mappingMatrix\latentMatrix + \meanVector\onesVector^\top + \noiseMatrix
\]
For probabilistic principal component analysis\index{principal component analysis (PCA)} the latent variables were assumed to be drawn from a spherical Gaussian \gls{prior_distribution},
\[
\latentScalar_{i, j} \sim \gaussianSamp{0}{1},
\]
that allows the nuisance variables, $\latentMatrix =
\left\{\latentVector_{i, :}\right\}_{i=1}^\numData$ to be marginalized analytically (\refsec{sec:ppca}) obtaining
\[
p(\dataMatrix|\mappingMatrix) = \gaussianDist{\dataVector_{i, :}}{\meanVector}{\mappingMatrix\mappingMatrix^\top + \sigma^2\eye},
\]
which is the marginal likelihood.

However, this constrained our data to be related to our latent
variables in a linear way. This constraint is very strong, and is
clearly often violated in practise. Nonlinear dimensionality reduction
involves assuming a nonlinear relationship between the latent
variables and each observed data dimension. So we have
\[
\dataScalar_{i, j} = \mappingFunction_j(\latentVector_{i, :}) +
\noiseScalar_{i, j}
\]
where $\noiseScalar_{i, j}$ is some corrupting noise, typically
independently and identically distributed. The linear special case of
assuming
\[
\mappingFunction_j(\latentVector) = \mappingVector_{j, :}^\top
\latentVector_{i, :} + \meanVector
\]
that we discussed in \refchap{chap:linear} combined with a Gaussian
assumption for the noise variables,
\[
\noiseScalar_{i, j} \sim \gaussianSamp{0}{\dataStd^2}
\]
%
\book{
%
Consider, for example, the rotated sixes example we introduced in \refchap{chap:mds}. They were projected onto their principal axes and a clear one dimensional nonlinear pattern, in the form of a circle was observed.
%
}{
%
\input{sixes}
} 
%
The data are one dimensional in terms of latent representation: they
were generated by a rotation of the original images which relied on a
singled parameter, the angle of rotation, but because we represented
them with a linear low dimensional space we required more dimensions
to represent the data. This motivates the need for nonlinear low
dimensional representations, before we consider this further though,
we first introduce a standard approach to representing nonlinear
functions through a \emph{\gls{basis}}.

\section{Basis Function Representations}

A common approach to regression is to specify that a function is given
by a linear sum over a fixed basis set,
\begin{equation}
  \mappingFunction\left(\inputVector_{i,:};\mappingVector\right) = \sum_{k=1}^\numBasisFunc \mappingScalar_k \basisFunction_k\left(\inputVector_{i,:}\right),\label{eq:parametricMapping}
\end{equation}
In this equation there are $\numBasisFunc$ basis functions, and the
$k$th basis function is represented by
$\basisFunction_k\left(\cdot\right)$ and
$\mappingVector=\left[\mappingScalar_1,\dots,\mappingScalar_\numBasisFunc\right]^\top$. Basis
functions can take several forms, the idea of the basis function is to
map the data into a feature space, from which a linear sum over the
basis leads to a non linear function. A common
\emph{\gls{local_basis}} is the \emph{\gls{radial_basis_function}}
where
\[
\basisFunction_k\left(\latentVector_i\right) = \exp\left( -\frac{\left\vert
      \latentVector_i - \boldsymbol{\mu}_k\right\vert^2}{2\rbfWidth^2}
\right),
\]
where each basis function is centered at $\boldsymbol{\mu}_k$, and has
radius of $\rbfWidth$. A set of such basis functions is visualized in
\reffig{fig:radialBasisFunctions}
\begin{figure}
  \begin{matlab}
    %}
    figure
    clf
    number = 200;
    numBasisFunc = 3;
    rbfWidth = 2;
    mu = linspace(-4, 4, numBasisFunc);
    x = linspace(-6, 6, number)';
    Phi = exp(-((repmat(x, 1, numBasisFunc) - repmat(mu, number, 1)).^2)/(2*rbfWidth*rbfWidth*rbfWidth));
    a = plot(x, Phi, '-');
    set(a, 'linewidth', 2);
    xlabel('$\inputScalar$', 'horizontalalignment', 'center')
    ylabel('$\basisFunction(\inputScalar)$', 'horizontalalignment', 'center')
    printLatexText(['\global\long\def\rbfWidthVal{' numsf2str(rbfWidth, 0) '}\global\long\def\meanVectorVal{' strrep(numsf2str(mu, 1), ' ', '\ ') '}'], 'basisFunctionPlotValues.tex', '../../../dimred/tex/diagrams');
    printLatexPlot('basisFunctionPlot', '../../../dimred/tex/diagrams', 0.65*textWidth);
    %{
  \end{matlab}
  \begin{center}
    \input{../../../dimred/tex/diagrams/basisFunctionPlot}
    \input{../../../dimred/tex/diagrams/basisFunctionPlotValues}
  \end{center}
  \caption{A set of radial basis functions with width
    $\rbfWidth=\rbfWidthVal$ and location parameters
    $\meanVector=[\meanVectorVal]^\top$.}\label{fig:radialBasisFunctions}
\end{figure}
is the center associated with the $k$th basis function.  Weighting
these basis functions using a matrix $\mappingMatrix$,
\[
\mappingFunction_j(\latentVector_i) = \sum_{k=1}^\numBasisFunc
\mappingScalar_{j,k}\phi_k(\latentVector_i)
\]
provides us with a function. To demonstrate the type of basis
functions shown in \reffig{fig:radialBasisFunctions} we can sample the
matrix $\mappingMatrix$ from a Gaussian density,
\[
\mappingScalar_{j, k} \sim \gaussianSamp{0}{\alpha},
\]
to give us the parameters. We can then visualize the resulting
functions by plotting them as in \reffig{fig:basisFunctionSamples}
\begin{figure}
  \begin{matlab}
    %}
    figure
    clf
    numSamples = 10;
    alpha = 1;
    W = randn(numSamples, numBasisFunc)*sqrt(alpha);
    F = Phi*W';
    a = plot(x, F, '-');
    set(a, 'linewidth', 2);
    set(gca, 'xlim', [-6, 6])
    xlabel('$\inputScalar$', 'horizontalalignment', 'center')
    ylabel('$\mappingFunction(\inputScalar)$', 'horizontalalignment', 'center')
    set(gca, 'ytick', [-2 -1 0 1 2])
    printLatexText(['\global\long\def\alphaVal{' numsf2str(alpha, 0) '}'], 'basisFunctionSamplesValues.tex', '../../../dimred/tex/diagrams');
    printLatexPlot('basisFunctionSamples', '../../../dimred/tex/diagrams', 0.65*textWidth);
    %{
  \end{matlab}
  \begin{center}
    \input{../../../dimred/tex/diagrams/basisFunctionSamples}
    \input{../../../dimred/tex/diagrams/basisFunctionSamplesValues}
  \end{center}
  \caption{Functions sampled using the basis set from
    \reffig{fig:radialBasisFunctions}. Each line is a separate sample,
    generated by a weighted sum of the basis set. The weights,
    $\mappingVector$ are sampled from a Gaussian density with variance
    $\alpha=\alphaVal$.}\label{fig:basisFunctionSamples}
\end{figure}

We can make use of these functions to map, for example, to map from a
$\latentDim=2$ to a $\dataDim=3$ dimensional space.

\begin{figure}
  \begin{matlab}
    %}
    clf
    numberAcross = 30;
    x1 = linspace(-1, 1, numberAcross);
    x2 = x1;
    mu1 = mu;
    mu2 = mu;
    [MU1, MU2] = meshgrid(mu1, mu2);
    [X1, X2] = meshgrid(x1, x2);
    X = [X1(:) X2(:)];
    MU = [MU1(:) MU2(:)];
    numBasisFunc = size(MU, 1);
    number = size(X, 1);
    Phi = exp(-dist2(X, MU)/(2*rbfWidth*rbfWidth));
    numSamples = 3;
    W = randn(numSamples, numBasisFunc)*sqrt(alpha);
    F = Phi*W';
    subplot(1, 3, 1)
    hold on
    startVal = 1;
    for i = 1:numberAcross
      endVal = numberAcross*i;
      a = plot(X(startVal:endVal, 1), X(startVal:endVal, 2), 'r-');
      startVal = endVal + 1;
    end
    % Reshape X to plot lines in opposite directions
    X1 = X1';
    X2 = X2';
    X = [X1(:) X2(:)];
    startVal = 1;
    for i = 1:numberAcross
      endVal = numberAcross*i;
      a = plot(X(startVal:endVal, 1), X(startVal:endVal, 2), 'r-');
      startVal = endVal + 1;
    end
    hold off
    axis equal
    axis off
    xlabel('$\inputScalar_1$', 'horizontalalignment', 'center')
    ylabel('$\inputScalar_2$', 'horizontalalignment', 'center')
  
    subplot(1, 3, 3)
    hold on
    startVal = 1;
    for i = 1:numberAcross
      endVal = numberAcross*i;
      a = plot3(F(startVal:endVal, 1), F(startVal:endVal, 2), F(startVal:endVal, 3), 'r-');
      startVal = endVal + 1;
    end
    % Reshape F to plot lines in opposite directions
    F1 = reshape(F(:, 1), size(X1, 1), size(X1, 2))';
    F2 = reshape(F(:, 2), size(X1, 1), size(X1, 2))';
    F3 = reshape(F(:, 3), size(X1, 1), size(X1, 2))';
    F = [F1(:) F2(:) F3(:)];
    startVal = 1;
    for i = 1:numberAcross
      endVal = numberAcross*i;
      a = plot3(F(startVal:endVal, 1), F(startVal:endVal, 2), F(startVal:endVal, 3), 'r-');
      startVal = endVal + 1;
    end
    hold off
    axis equal
    axis off
    % Treble axis size to increase plot size
    pos = get(gca, 'position');
    npos = pos;
    npos(3:4) = pos(3:4)*3;
    npos(1) = pos(1) - 0.5*(npos(3)-pos(3));
    npos(2) = pos(2) - 0.5*(npos(4)-pos(4));
    set(gca, 'position', npos);
    % pause

    % Axis for writing text on plot
    axes('position', [0 0 1 1])
    set(gca, 'xlim', [0 1])
    set(gca, 'ylim', [0 1])
    axis off
    text(0.5, 0.55, '\large$\dataScalar_j = \mappingFunction_j(\inputVector)$', 'horizontalalignment', 'center')
    text(0.5, 0.45, '\LARGE$\longrightarrow$', 'horizontalalignment', 'center')


    options = printLatexOptions;
    options.maintainAspect = false;
    options.height = 0.3*textWidth;
    printLatexPlot('nonlinearMapping3DPlot', '../../../dimred/tex/diagrams', 0.95*textWidth, options);

    %{
  \end{matlab}
  \begin{center}
    \input{../../../dimred/tex/diagrams/nonlinearMapping3DPlot}
  \end{center}
  \caption{A three dimensional manifold formed by mapping from a two dimensional space to a three dimensional space.}\label{fig:sampleManifold}
\end{figure}


\begin{figure}
  \begin{matlab}
    %}
    clf
    numberAcross = 101;
    x = linspace(-6, 6, numberAcross)';
    numBasisFunc = size(mu, 2);
    number = size(x, 1);
    Phi = exp(-dist2(x, mu')/(2*rbfWidth*rbfWidth));
    F = Phi*W(1:2, 1:numBasisFunc)';
    subplot(1, 3, 1)
    a = plot(x, ones(size(x, 1), size(x, 2)), 'r-');
    hold on
    subx = x(1:10:end);
    b = plot(subx, ones(size(subx, 1), size(subx, 2)), 'bx');
    set(gca, 'ylim', [-0.9 1.1])
    axis equal
    set(a, 'linewidth', 3);
    set(b, 'markersize', 10);
    axis off
    % Reshape X to plot lines in opposite directions
    xlabel('$\latentScalar$', 'horizontalalignment', 'center')

    
    subplot(1, 3, 3)
    a = plot(F(:, 1), F(:, 2), 'r-');
    hold on
    b = plot(F(1:10:end, 1), F(1:10:end, 2), 'bx');
    set(a, 'linewidth', 3)
    set(b, 'markersize', 10);
    xlabel('$\dataScalar_1$')
    ylabel('$\dataScalar_2$')
    hold off
    axis equal
    axis off

    % Axis for writing text on plot
    axes('position', [0 0 1 1])
    set(gca, 'xlim', [0 1])
    set(gca, 'ylim', [0 1])
    axis off
    text(0.5, 0.65, '\large$\dataScalar_1 = \mappingFunction_1(\latentScalar)$', 'horizontalalignment', 'center')
    text(0.5, 0.5, '\LARGE$\longrightarrow$', 'horizontalalignment', 'center')
    text(0.5, 0.35, '\large$\dataScalar_2 = \mappingFunction_2(\latentScalar)$', 'horizontalalignment', 'center')


    options = printLatexOptions;
    options.maintainAspect = false;
    options.height = 0.3*textWidth;
    printLatexPlot('nonlinearMapping2DPlot', '../../../dimred/tex/diagrams', 0.95*textWidth, options);

    %{
  \end{matlab}
  \begin{center}
    \input{../../../dimred/tex/diagrams/nonlinearMapping2DPlot}
  \end{center}
  \caption{A string in two dimensions, formed by mapping from one
    dimension, $\latentScalar$, line to a two dimensional space,
    $[\dataScalar_1,\ \dataScalar_2]$ using nonlinear functions
    $\mappingFunction_1(\cdot)$ and
    $\mappingFunction_2(\cdot)$.}
  \label{fig:sampleManifold2d}
\end{figure}

By the probabilistic model 


can then be specified by adding noise,
\[
\dataScalar\left(\latentVector_i\right) = \mappingFunction\left(\latentVector_i; \mappingVector\right) +
\epsilon_i,
\]
where $\epsilon_i$ is the noise associated with the $i$th data
point. If the noise is taken to be Gaussian distributed with variance
$\dataStd^2$,
\[
\epsilon_i\sim\gaussianSamp{0}{\dataStd^2},
\]
then we can write down the following probabilistic representation for
our data,
\[
p\left(\dataVector|\latentMatrix, \mappingVector, \dataStd^2\right) =
\prod_{i=1}^\numBasisFunc\gaussianDist{\dataScalar_i}{\mappingScalar_i}{\dataStd^2},
\]
where the mean of the Gaussian distributions in the product is given
by the evaluations of our function at the training points,
$\mappingScalar_i=\mappingFunction\left(\latentVector_i; \mappingVector\right)$.

This looks very similar to the representation we used in
\refsec{}. However, there is one important difference. We have
made use of a parameter vector in the representation above. To
determine the parameters, we might want to use Bayesian inference, for
computational convenience placing a zero mean Gaussian prior over
$\mappingVector$ with covariance matrix $\gamma^\prime\eye$,
\[
p\left(\mappingVector\right) = \gaussianDist{\mappingVector}{\zerosVector}{\gamma^\prime \eye}.
\]
By constructing a design matrix from our basis functions,
$\basisMatrix=\left[\basisVector_1, \dots,
  \basisVector_m\right]$, where
$\basisVector_j=\left[\basisFunction_j\left(\latentVector_1\right), \dots,
  \basisFunction_j\left(\latentVector_\numData\right)\right]^\top$ is the vector
containing the values of the $j$th basis function at all input data
points, we can rewrite (\ref{eq:parametricMapping}) for the training
data in matrix vector form,
\[
\mappingFunctionVector = \basisMatrix\mappingVector.
\]

\section{Nonlinear Probabilistic Approaches and Normalization}

The difficulty for probabilistic approaches to dimensionality
reduction is that the generative model the proscribe is difficult to
normalize. If we assume that the prior distribution is Gaussian,
\[
\latentVector_{i, :} \sim \gaussianSamp{\zerosVector}{\eye}
\]
and that the mapping for each data point is given by 
\[
\dataScalar_{i, j} = \mappingFunction_j(\latentVector_{i, :}) + \noiseScalar_{i, j}
\]
with a standard Gaussian noise assumption,
\[
\noiseScalar_{i, j} \sim \gaussianSamp{\zerosVector}{\dataStd^2}
\]
then the likelihood of the data given the latent variables is given by
\[
p(\dataVector_{i, :}| \latentVector_{i, :}) =
\prod_{j=1}^\dataDim\gaussianDist{\dataScalar_{i,
    j}}{\mappingFunction_j(\latentVector_{i, :})}{\dataStd^2\eye}.
\]
The marginalized likelihood of the data then has the form
\begin{align*}
  p(\dataVector_{i, :}) = &\int p(\dataVector_{i, :}, \latentVector_{i, :})\text{d}\latentVector_{i, :}\\
  & \int p(\dataVector_{i, :}|\latentVector_{i, :}) p(\latentVector_{i, :})\text{d}\latentVector_{i, :}\\
  & \int \prod_{j=1}^\dataDim\gaussianDist{\dataScalar_{i,
      j}}{\mappingFunction_j(\latentVector_{i,
      :})}{\dataStd^2}\gaussianDist{\latentVector_{i,
      :}}{\zerosVector}{\eye}\text{d}\latentVector_{i, :}.
\end{align*}
The key component of this integral is in the exponent of the joint
density. Taking the logarithm we can see that the exponent has the
form
\[
\log p(\dataVector_{i, :}, \latentVector_{i, :}) =
-\frac{1}{2}\sum_{j=1}^\dataDim(\dataScalar_{i, j} -
\mappingFunction_j(\latentVector_{i, :}))^2
-\frac{1}{2}\latentVector_{i, :}^\top \latentVector_{i, :} +
\text{const}
\]
where we have introduced a constant term that does not depend on $\latentVector_{i, :}$. For the linear case, where we have
$\mappingFunction_j(\latentVector)=\mappingVector_{j,
  :}^\top\latentVector$, we can rewrite this exponent as
\[
\log p(\dataVector_{i, :}, \latentVector_{i, :}) =
-\frac{1}{2}\dataVector_{i, :}^\top\dataVector_{i, :} +
\dataVector_{i, :}^\top\mappingMatrix\latentVector_{i, :} -
\frac{1}{2}\latentVector_{i,
  :}^\top\mappingMatrix^\top\mappingMatrix\latentVector_{i, :}
-\frac{1}{2}\latentVector_{i, :}^\top \latentVector_{i, :} +
\text{const}
\]
which is a quadratic form implying that integrating over
$\latentVector_{i, :}$ to find the marginal likelihood
$p(\dataVector_{i, :})$ \emph{is} tractable. This is the integral that is
performed in probabilistic PCA and factor analysis (see
\refchap{chap:linear}).  However, for more general nonlinear
functions,\footnote{There are some specific nonlinear functions which
  keep the integral tractable. For example if
  $\dataScalar=\exp(\latentScalar)$ we can perform the integral and
  the resulting density over $\dataScalar$ is known as the
  \emph{\gls{log_normal}}.}  $\mappingFunction(\cdot)$, this integral
is \emph{not} tractable. The motivation for using general nonlinear
functions is that they do lead to a much richer class of probability
densities, for example, even in the special case where we don't
perform any dimensionality reduction, for example $\dataDim=1$ and
$\latentDim=1$, we can plot the distribution over $\latentScalar$ and
make a numerical estimate of the corresponding density over
$\dataScalar$. This is done in
\reffig{fig:gaussianThroughNonlinear}. Placing the density through the
nonlinear function leads to a more complex multimodal
density. However, the price we pay for the greater representational
power of the model is the inability to express the marginal likelihood
for $p(\dataVector_{i, :})$ in a closed form, to fit such models we
need to look for approximations.
% 
\begin{figure}
  \begin{matlab}
    %}
    clf
    dataStd = 0.2;
    numSamples = 10000;
    xsamp = randn(numSamples, 1);

    % Create RBF network with much larger variation in functions.
    mu = linspace(-4, 4, 100);
    numBasisFunc = size(mu, 2);
    rbfWidth = 0.1;
    Phi = exp(-dist2(xsamp, mu')/(2*rbfWidth*rbfWidth));
    W = randn(1, numBasisFunc)*sqrt(alpha);
    f = Phi*W';
    subplot(1, 3, 1)
    hold on
    p = exp(-0.5/alpha*x.^2)*1/sqrt(2*pi*alpha);
    a = patch(x, p, patchColor);
    axis off
    set(a, 'linewidth', 2);
    set(gca, 'ylim', [0 0.5]);
    set(gca, 'xlim', [-6 6]);
    xlabel('$p(\latentScalar)$', 'horizontalalignment', 'center')
    
    subplot(1, 3, 3)
    y = linspace(min(f)-3*dataStd, max(f)+3*dataStd, 100)';
    p = mean(exp(-0.5/(dataStd*dataStd)*dist2(y, f))*1/(sqrt(2*pi)*dataStd), 2);
    a = patch(y, p, patchColor);
    axis off
    set(a, 'linewidth', 2);
    set(gca, 'ylim', [0 0.5])
    set(gca, 'xlim', [-6 6]);
    xlabel('$p(\dataScalar)$', 'horizontalalignment', 'center')

    % Axis for writing text on plot
    axes('position', [0 0 1 1])
    set(gca, 'xlim', [0 1])
    set(gca, 'ylim', [0 1])
    axis off
    text(0.5, 0.45, '\large$\dataScalar = \mappingFunction(\latentScalar) + \noiseScalar$', 'horizontalalignment', 'center')
    text(0.5, 0.35, '\LARGE$\longrightarrow$', 'horizontalalignment', 'center')

    options = printLatexOptions;
    options.maintainAspect = false;
    options.height = 0.3*textWidth;
    printLatexPlot('gaussianThroughNonlinear2', '../../../dimred/tex/diagrams', 0.95*textWidth, options);
    printLatexText(['\global\long\def\rbfWidthVal{' numsf2str(rbfWidth, 0) '}\global\long\def\dataStdVal{' strrep(numsf2str(dataStd, 1), ' ', '\ ') '}'], 'gaussianThroughNonlinear2Values.tex', '../../../dimred/tex/diagrams');
    %{
  \end{matlab}
  \begin{centering}
    \input{../../../dimred/tex/diagrams/gaussianThroughNonlinear2}
    \input{../../../dimred/tex/diagrams/gaussianThroughNonlinear2Values}
  \end{centering}

  \caption{A Gaussian distribution propagated through a non-linear mapping. We define a one dimensional Gaussian distribution in $\latentScalar$ (left). Then the relationship between $\dataScalar$ and $\latentScalar$ is defined as $\dataScalar_i=\mappingFunction(\latentScalar_i) + \noiseScalar_i$ where $\noiseScalar$ is Gaussian random noise with standard deviation $\dataStd=\dataStdVal$ and $\mappingFunction(\cdot)$ is computed using an RBF basis with 100 centres uniformly distributed between -4 and 4 and basis function widths $\rbfWidth=\rbfWidthVal$. The new distribution over $\dataScalar$ (right) is multimodal and difficult to normalize. It is this normalization problem that provides problems for models based on such nonlinear functions.}\label{fig:gaussianThroughNonlinear}

\end{figure}


\section{Density Networks}

Density networks were introduced by \cite{MacKay:wondsa95} as an
approximation for fitting such models. Density networks make use of a
sample based approximation to the marginal likelihood to represent the
data. Making explicit dependence of the mapping function,
$\mappingFunction_{j}(\latentVector_{i, :};\ \paramVector)$, on its
parameters, $\paramVector$, we have
\[
p\left(\dataMatrix|\latentMatrix\right)=\prod_{i=1}^{\numData}\prod_{j=1}^{\dataDim}\gaussianDist{\dataScalar_{i,j}}{\mappingFunction_{j}(\latentVector_{i,:};\ \paramVector)}{\dataStd^{2}}
\]
and typically we would chose a Gaussian prior for the latent space
(although we can choose any distribution here from which we can sample),
\[
  p\left(\latentMatrix\right)=\gaussianDist{\latentVector_{i,:}}{\zerosVector}{\eye}
\]

\subsection{Sample Based Approximation}

The sample based approximation involves replacing the continuous integral over the density with a discrete sum of samples from the density. Given a set of $\numComponents$ samples, $\left\{\hat{\latentVector}_{s, :}\right\}_{s=1}^\numComponents$ drawn from $p(\latentVector_{i, :})$ we can write down the sample based approximation to the marginal likelihood for the $i$th data point as
\begin{align*}
p(\dataVector_{i, :}) & = \int \prod_{j=1}^\dataDim p\left(\dataScalar_{i,j}| \latentVector_{i,:}, \paramVector\right) p(\latentVector_{i,:}) \text{d}\latentVector_{i, :}\\
 &\approx \frac{1}{\numComponents} \sum_{s=1}^{\numComponents}\prod_{j=1}^\dataDim p\left(\dataScalar_{i,j}| \hat{\latentVector}_{s,:}, \paramVector\right) 
\end{align*}
where we have been explicit about including the parameter vector, $\paramVector$, in the likelihood and the approximation becomes more accurate as our sample size, $\numComponents$, goes towards infinity\book{, see \refbox{box:sample} for more details on sample based approximations}{}.
\begin{boxfloat}
\caption{Sample Based Approximations}\label{box:sample}
\end{boxfloat}
The joint likelihood of the entire data set is given by a product over the likelihoods for each data point,
\[
p(\dataMatrix|\paramVector) = \prod_{i=1}^\numData p(\dataVector_{i, :}|\paramVector)
\]
reflecting an assumption that the data is independent and identically distributed \emph{given} the parameters, $\paramVector$. So to compute the likelihood of the entire data set, $\dataMatrix$, we need to consider sample based approximations for every data point. The key innovation in density networks is to use the \emph{same set} of samples for each data point. This turns out to significantly reduce the computational demands. Taking the logarithm and substituting with our approximation we have
\[
\log p\left(\dataMatrix|\paramVector\right)=\sum_{i=1}^{\numData}\log\frac{1}{\numComponents}\sum_{s=1}^{\numComponents}p\left(\dataVector_{i,:}|\hat{\latentVector}_{s,:}, \paramVector\right).
\]
\subsection{Maximizing the Approximate Likelihood}

We can directly maximize the approximate likelihood by taking its gradients with respect to the parameters, $\paramVector$,
\[
\diff{}{\paramVector} \log p\left(\dataVector_{i,:}|\paramVector\right)=\sum_{s=1}^{\numComponents}\frac{p\left(\dataVector_{i,:}|\hat{\latentVector}_{s,:}, \paramVector,\right)}{\sum_{s^\prime=1}^{\numComponents}p\left(\dataVector_{i,:}|\hat{\latentVector}_{s^\prime,:}, \paramVector\right)}\diff{}{\paramVector}\log p\left(\dataVector_{i,:}|\hat{\latentVector}_{s,:}, \paramVector\right).
\]
For convenience we define,
\[
\responsibility_{i, s}= \frac{p\left(\dataVector_{i,:}|\paramVector,\hat{\latentVector}_{s,:}\right)}{\sum_{s^\prime=1}^{\numComponents}p\left(\dataVector_{i,:}|\paramVector,\hat{\latentVector}_{s^\prime,:}\right)},
\]
which, as we will see has multiple interpretations depending on the context. These include the posterior probability of mixture component membership (sometimes referred to as responsibility) and importance sampling weights\book{, see \refbox{box:importance}}{}. This allows us to write the gradient of the log likelihood as a weighted sum of gradients of the likelihood as follows 
\[
\diff{}{\paramVector}\log p\left(\dataVector_{i,:}|\paramVector\right)=\sum_{s=1}^{\numComponents}\responsibility_{i,s}\diff{}{\paramVector}\log p\left(\dataVector_{i,:}|\paramVector,\hat{\latentVector}_{s,:}\right).
\]


% 
\begin{boxfloat}
\caption{Importance Sampling}\label{box:importance}

\end{boxfloat}



\subsection{Density Network Mappings}

The formalism for density networks allows any general
$\mappingFunction(\cdot)$ but at the time of publication
\emph{\gls{multilayer_perceptron}} models were particularly popular so
it probably seemed natural to consider functions based on these
models. These can be seen as basis function models where each basis
functions is given by
\[
\basisFunction_k(\latentVector_{i, :}; \mappingVectorTwo_{k, :}) = \frac{1}{1+\exp(-\mappingVectorTwo_{k, :}^\top\latentVector_{i, :})}.
\]
The parameters of the basis functions, $\mappingMatrixTwo =
\left\{\mappingVectorTwo_{k, :}\right\}_{k=1}^{\numBasisFunc}$, can be
optimized along with the matrix $\mappingMatrix$.


\begin{figure}
  \centering{}\includegraphics[width=0.8\textwidth]{../diagrams/gaussianThroughNonLinear2d}\caption{One dimensional Gaussian mapped to two dimensions.}

\end{figure}


% 
\begin{figure}
  \centering{}\includegraphics[width=0.9\textwidth]{../diagrams/gaussianThroughNonLinear3d}
  \caption{Two dimensional Gaussian mapped to three dimensions.}

\end{figure}



\section{Likelihood Optimisation}


\subsection{Example: Oil Data}


% 
\begin{figure}
  \begin{centering}
    \subfigure[]{
      \includegraphics[width=0.45\textwidth]{../../../dimred/tex/diagrams/demOilDnet4NoGray}}\subfigure[]{
      \includegraphics[width=0.45\textwidth]{../../../dimred/tex/diagrams/demOilDnet5NoGray}}
  \end{centering}
  
  \caption{Oil data visualised with a density network using an MLP network with
    (a) 100 (b) 400 points in the sample. Nearest neighbour errors: (a)
    22 (b)16. Code can be run with (a) \texttt{demOilDnet4} (b) \texttt{
      demOilDnet5}}

\end{figure}



\section{Generative Topographic Mapping}


Generative Topographic Mapping (GTM) \cite{Bishop:gtm_ncomp98} 

Key idea: Lay points out on a \emph{grid.}

Constrained mixture of Gaussians.

\begin{figure}


  \begin{center}
    \includegraphics[width=0.8\textwidth]{../../../dimred/tex/diagrams/gaussianThroughNonLinearGTM3d}
  \end{center}
  \caption{One dimensional Gaussian mapped to two dimensions.}
  
\end{figure}


\subsection{ GTM Prior}

Prior distribution is a mixture model in a latent space.
  \[
  p\left(\latentMatrix\right)=\prod_{i=1}^{\numData}p\left(\latentVector_{i,:}\right)
  \]
  \[
  p\left(\latentVector_{i,:}\right)=\frac{1}{\numComponents}\sum_{s=1}^{\numComponents}\delta\left(\latentVector_{i,:}-\hat{\latentVector}_{s,:}\right)
  \]

The $\hat{\latentVector}_{s,:}$ are laid out on a regular grid.

\subsection{Mapping and E-Step}

Likelihood is a Gaussian with non-linear mapping from latent space
  to data space for the mean
  \[
  p\left(\dataMatrix|\latentMatrix,\paramVector\right)=\prod_{i=1}^{\numData}\prod_{j=1}^{\dataDim}\gaussianDist{\dataScalar_{i,j}}{\mappingScalar_{j}\left(\latentVector_{i,:};\mappingMatrix,\rbfWidth\right)}{\dataStd^{2}}
  \]
  In the original paper \cite{Bishop:gtmncomp98} an RBF network was
  suggested,


In the E-step, posterior distribution over $k$ is given by 
  \[
  \responsibility_{i,k}=\frac{\prod_{j=1}^{\dataDim}\gaussianDist{\dataScalar_{i,j}}{\mappingFunction_{j}\left(\hat{\latentVector}_{k};\mappingMatrix,\rbfWidth\right)}{\dataStd^{2}}}{\sum_{s=1}^{\numComponents}\prod_{j=1}^{\dataDim}\gaussianDist{\dataScalar_{i,j}}{\mappingFunction_{j}\left(\hat{\latentVector}_{s};\mappingMatrix,\rbfWidth\right)}{\dataStd^{2}}}
  \]
sometimes called the ``responsibility of component $k$ for data
  point $i$''.

\subsection{Likelihood Optimisation}

We then maximise the lower bound on the log likelihood,
\[
  \log p\left(\dataVector_{i,:}|\paramVector\right)\geq\left\langle \log p\left(\dataVector_{i,:},\hat{\latentVector}_{s,:}|\paramVector\right)\right\rangle _{q\left(s\right)}-\left\langle \log q\left(s\right)\right\rangle _{q\left(s\right)},\]

Free energy part of bound 
  \[
  \left\langle \log p\left(\dataVector_{i,:},\hat{\latentVector}_{s,:}|\paramVector\right)\right\rangle =\sum_{s=1}^{\numComponents}\responsibility_{i,s}\log p\left(\dataVector_{i,:}|\hat{\latentVector}_{s,:},\paramVector\right)+\text{const}
  \]

When optimising parameters in EM, we ignore dependence of $\responsibility_{i,k}$
  on parameters. So we have
  \[
  \diff{}{\paramVector}\left\langle \log p\left(\dataVector_{i,:},\hat{\latentVector}_{s,:}|\paramVector\right)\right\rangle =\sum_{s=1}^{\numComponents}\responsibility_{i,s}\diff{}{\paramVector}\log p\left(\dataVector_{i,:}|\hat{\latentVector}_{s,:},\paramVector\right)
  \]
  which is very similar to density network result!  Interpretation of
  posterior is slightly different.


\subsection{Oil Data}

% 
\begin{figure}
  \begin{centering}
    \subfigure[]{
      \includegraphics[width=0.33\textwidth]{../../../dimred/tex/diagrams/demOilDnet1NoGray}}
    \subfigure[]{\includegraphics[width=0.33\textwidth]{../../../dimred/tex/diagrams/demOilDnet2NoGray}}
    \subfigure[]{
      \includegraphics[width=0.33\textwidth]{../../../dimred/tex/diagrams/demOilDnet3NoGray}}
  \end{centering}

  \caption{Oil data visualised with the GTM using an RBF network with
    (a) 10$\times$10 (b) $20\times20$ and (c) $30\times30$ points in
    the grid. Nearest neighbour errors: (a) 74 (b) 44 (c) 11. These
    experiments can be recreated with (a) \texttt{demOilDnet1} (b)
    \texttt{demOilDnet2} (c) \texttt{demOilDnet3}.}

\end{figure}
\subsection{Magnification Factors}

\cite{Bishop:iee_mag97}

\subsection{Stick Man Data}

% 
\begin{figure}
  \begin{centering}
    \subfigure[]{
      \includegraphics[width=0.33\textwidth]{../../../dimred/tex/diagrams/demStickDnet1NoGray}}
    \subfigure[]{
      \includegraphics[width=0.33\textwidth]{../../../dimred/tex/diagrams/demStickDnet2NoGray}}
    \subfigure[]{
      \includegraphics[width=0.33\textwidth]{../../../dimred/tex/diagrams/demStickDnet3NoGray}}
  \end{centering}

  \caption{Oil data visualised with the GTM using an RBF network with (a) 10$\times$10
    (b) $20\times20$ (c) $30\times30$ points in the grid. Experiments
    can be recreated with (a) \texttt{demStickDnet1} (b) \texttt{demStickDnet2}
    (c) \texttt{demStickDnet3}.}
  
\end{figure}




\subsection{Separated Means: Bubblewrap Effect}

% 
\begin{figure}


  \begin{centering}
    \subfigure[]{
      \includegraphics[width=0.49\textwidth]{../diagrams/bubblewrap}}\hfill{}
    \subfigure[]{
      \includegraphics[width=0.49\textwidth]{../diagrams/bubblewrapNonlinear}}
    \par\end{centering}

  \caption{The manifold is more like bubblewrap than a piece of paper.}



\end{figure}



% 
\begin{figure}
  \begin{centering}
    \subfigure[]{
      \includegraphics[width=0.8\textwidth]{../diagrams/gaussianPosteriors1sd}}
    \par\end{centering}

  \begin{centering}
    \subfigure[]{
      \includegraphics[width=0.8\textwidth]{../diagrams/gaussianPosteriors4sd}}
    \par\end{centering}

  \begin{centering}
    \subfigure[]{

      \includegraphics[width=0.8\textwidth]{../diagrams/gaussianPosteriors16sd}}
    \par\end{centering}

  \caption{As Gaussians become further apart the posterior probability becomes
    more abrupt. (a) 1 (b) 4 (c) 16 standard deviations apart. }

\end{figure}


\subsection{Equivalence of GTM and Density Networks}

GTM and Density Networks have the same origin. \cite{Bishop:emdn95,MacKay:wondsa95}.

In original Density Networks paper MacKay suggested Importance Sampling
  \cite{MacKay:wondsa95}.

Early work on GTM also used importance sampling.

Main innovation in GTM was to lay points out on a grid (inspired by
  Self Organizing Maps \cite{Kohonen:book01}.

\section{Non Linear Factor Analysis}

Variational approach to dimensionality reduction.

Combine Gaussian prior over latent space with neural network \cite{Honkela:unsupervised04}

Assume variational prior separates.
Optimise with respect to variational distributions.

\subsection{Summary}

Two point based approaches to dimensionality reduction.

Approaches seem to generalise well even when dimensions of data is
  greater than number of points.

Approaches are difficult to extend to higher dimensional latent
  spaces: number of samples/centres required increases exponentially with dimension.

Next we will explore a different probabilistic interpretation of PCA
  and extend that to non-linear models.
%}

%%% Local Variables:
%%% TeX-master: "book"
%%% End:
